{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get preprocessed 'DontPatronizeMe' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-gNxTZfDL0aOpzOnxE80M29dUVjSoozn' -O 'train.csv'\n",
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh' -O 'valid.csv'\n",
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-13l35-18IYPFSV_36llsJbb7c4Gu2o0' -O 'test.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from denoising_diffusion_pytorch import GaussianDiffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pandas data\n",
    "train_path = './train.csv'\n",
    "valid_path = './valid.csv'\n",
    "test_path = './test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_path).dropna()\n",
    "valid_df = pd.read_csv(valid_path).dropna()\n",
    "test_df = pd.read_csv(test_path).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.save_pretrained(\"./tokenizers/distilbert-base-uncased-local\")\n",
    "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.save_pretrained(\"./models/distilbert-base-uncased-local/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "max_length = 256 # max text length\n",
    "learning_rate = 0.01\n",
    "beta_min = 0.0001\n",
    "beta_max = 0.02\n",
    "step_tot = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"./models/distilbert-base-uncased-local\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# configuration = DistilBertConfig()\n",
    "# print(configuration)\n",
    "betas = torch.linspace(beta_min, beta_max, step_tot)\n",
    "alphas = 1 - betas\n",
    "alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
    "def diffuse_t(x, t):\n",
    "  noise = torch.normal(0, 1, x.shape)\n",
    "  return torch.sqrt(alpha_cumprod[t]) * x + noise * torch.sqrt(1 - alpha_cumprod[t])\n",
    "\n",
    "def loss(model, x, mask, t, loss_func):\n",
    "  '''\n",
    "  model: torch model accept x shape as input\n",
    "  x: x_0\n",
    "  alpha_cumprod: bar_alpha list\n",
    "  loss_func: \"l1\" or \"l2\" loss function between x_0 and predicted x_0\n",
    "  '''\n",
    "  noised = diffuse_t(x, t)\n",
    "  x_0_hat = model(input_ids=noised, attention_mask=mask, output_hidden_states=True)[1][0]\n",
    "  return loss_func(x_0_hat, x)\n",
    "\n",
    "\n",
    "# Initializing a model from the configuration\n",
    "model = DistilBertForMaskedLM.from_pretrained(\"./models/distilbert-base-uncased-local\", local_files_only=True)\n",
    "embedding = model.get_input_embeddings().requires_grad_(False)\n",
    "model.set_input_embeddings(nn.Sequential())\n",
    "print(model.config)\n",
    "\n",
    "trainer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "class DPMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, input_df, embedding):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = input_df['text'].tolist()\n",
    "        self.embedding = embedding\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # function for batch allocation\n",
    "        texts = []\n",
    "\n",
    "        for b in batch:\n",
    "            texts.append(b)\n",
    "\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "        return {\"embeddings\": self.embedding(encodings[\"input_ids\"]), \"attention_mask\": encodings[\"attention_mask\"]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"./tokenizers/distilbert-base-uncased-local\", local_files_only=True)\n",
    "train_dataset = DPMDataset(tokenizer, train_df, embedding)\n",
    "# train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished one epoch training\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "  for t in range(1, step_tot + 1, 30):\n",
    "    trainer.zero_grad()\n",
    "    l = loss(model, x[\"embeddings\"], x[\"attention_mask\"], t, nn.L1Loss())\n",
    "    l.backward()\n",
    "    trainer.step()\n",
    "  print(\"finished one epoch training\")\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text fetched torch.Size([16, 98, 768])\n",
      "noise added\n",
      "inference finished\n",
      "origin text:  Critics have even taken to dobbing in Katrina Bungard to National Party leader Bill English when they see her sign-written car bearing her name and photo parked in disabled parks .\n",
      "inferred:  ,,, some,,,, endless endless,,,,,, endless,, endless,,,, some endless,, endless endless, endless endless,,,, some some,, endless endless,, endless endless,,,,, endless,,, some some,,,,,,, endless some,,, some endless, some,,,,,,,, endless, endless, some endless,,,, some endless,, some,\n"
     ]
    }
   ],
   "source": [
    "# trial on inference\n",
    "for text in train_loader:\n",
    "  break\n",
    "print(\"text fetched\", text[\"embeddings\"].shape)\n",
    "noised_text = diffuse_t(text[\"embeddings\"], 100)\n",
    "print(\"noise added\")\n",
    "restored = model(noised_text, text[\"attention_mask\"], output_hidden_states=True)\n",
    "print(\"inference finished\")\n",
    "print(\"origin text: \", train_df.loc[0][\"text\"])\n",
    "print(\"inferred: \", tokenizer.decode(torch.softmax(restored[0][0], dim=-1).argmax(dim=-1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
