{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-nO5MNtdS1v"
      },
      "source": [
        "# Get preprocessed 'DontPatronizeMe' dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XS1i5FqdS1x",
        "outputId": "991bdbb8-5f10-4fa5-db41-8d26deaf1ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-19 15:04:57--  https://drive.google.com/uc?export=download&id=1-gNxTZfDL0aOpzOnxE80M29dUVjSoozn\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.100, 173.194.203.138, 173.194.203.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... ^C\n",
            "--2022-08-19 15:04:58--  https://drive.google.com/uc?export=download&id=1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.100, 173.194.203.138, 173.194.203.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j1lhliqq1hhsvanps0a2iu24hvtg09f7/1660921425000/09836793732558009118/*/1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh?e=download&uuid=639aab9b-351c-4fdf-b62c-61c6ea811213 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-08-19 15:04:59--  https://doc-10-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j1lhliqq1hhsvanps0a2iu24hvtg09f7/1660921425000/09836793732558009118/*/1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh?e=download&uuid=639aab9b-351c-4fdf-b62c-61c6ea811213\n",
            "Resolving doc-10-9s-docs.googleusercontent.com (doc-10-9s-docs.googleusercontent.com)... 74.125.199.132, 2607:f8b0:400e:c02::84\n",
            "Connecting to doc-10-9s-docs.googleusercontent.com (doc-10-9s-docs.googleusercontent.com)|74.125.199.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 483768 (472K) [text/csv]\n",
            "Saving to: ‘valid.csv’\n",
            "\n",
            "valid.csv           100%[===================>] 472.43K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-08-19 15:04:59 (116 MB/s) - ‘valid.csv’ saved [483768/483768]\n",
            "\n",
            "--2022-08-19 15:04:59--  https://drive.google.com/uc?export=download&id=1-13l35-18IYPFSV_36llsJbb7c4Gu2o0\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.100, 173.194.203.138, 173.194.203.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nkib5pt8p4l4grhsch447heqd5k9dcoo/1660921500000/09836793732558009118/*/1-13l35-18IYPFSV_36llsJbb7c4Gu2o0?e=download&uuid=b9688887-59d3-4677-8a68-b8e2f26defbe [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-08-19 15:05:00--  https://doc-00-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nkib5pt8p4l4grhsch447heqd5k9dcoo/1660921500000/09836793732558009118/*/1-13l35-18IYPFSV_36llsJbb7c4Gu2o0?e=download&uuid=b9688887-59d3-4677-8a68-b8e2f26defbe\n",
            "Resolving doc-00-9s-docs.googleusercontent.com (doc-00-9s-docs.googleusercontent.com)... 74.125.199.132, 2607:f8b0:400e:c02::84\n",
            "Connecting to doc-00-9s-docs.googleusercontent.com (doc-00-9s-docs.googleusercontent.com)|74.125.199.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 602497 (588K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 588.38K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-08-19 15:05:00 (124 MB/s) - ‘test.csv’ saved [602497/602497]\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
            "    from pip._internal.cli.parser import ConfigOptionParser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
            "    from pip._internal.configuration import Configuration, ConfigurationError\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/configuration.py\", line 21, in <module>\n",
            "    from pip._internal.exceptions import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/exceptions.py\", line 7, in <module>\n",
            "    from pip._vendor.pkg_resources import Distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 83, in <module>\n",
            "    __import__('pip._vendor.packaging.specifiers')\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-gNxTZfDL0aOpzOnxE80M29dUVjSoozn' -O 'train.csv'\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh' -O 'valid.csv'\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-13l35-18IYPFSV_36llsJbb7c4Gu2o0' -O 'test.csv'\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import package, model, dataset"
      ],
      "metadata": {
        "id": "GH5doKeWoxgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U6xCt976dS1z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "090d8988-39aa-4738-b185-37cd2d0f2717"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c20ca84fb72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "from transformers import (\n",
        "    DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
        ")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB9f_6mxdS1z"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  dev = \"cuda:0\"\n",
        "else:\n",
        "  dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print(\"using device: \", dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbAapRyQdS10"
      },
      "outputs": [],
      "source": [
        "# read pandas data\n",
        "train_path = './train.csv'\n",
        "valid_path = './valid.csv'\n",
        "test_path = './test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path).dropna()\n",
        "valid_df = pd.read_csv(valid_path).dropna()\n",
        "test_df = pd.read_csv(test_path).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dgq6KK1dS10"
      },
      "outputs": [],
      "source": [
        "# download model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer.save_pretrained(\"./tokenizers/distilbert-base-uncased-local\")\n",
        "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
        "model.save_pretrained(\"./models/distilbert-base-uncased-local/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drw9XZ_GdS11"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "chPgGq3bdS12"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 16\n",
        "max_length = 64 # max text length\n",
        "learning_rate = 1e-4\n",
        "epoch_num = 10\n",
        "beta_min = 0.0001\n",
        "beta_max = 0.02\n",
        "step_tot = 2000 # total noise adding steps\n",
        "sample_size = 1 # number of sample steps in each diffuse sequence\n",
        "train_embedding = False # if embedding layer is trainable\n",
        "x_0_prediction = False # if model predicts x_0 or x_{t-1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZwRatWVdS13"
      },
      "source": [
        "# Model, trainer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA6YHlL7jIt-",
        "outputId": "cc634fb2-8429-4086-aba3-cf311f66b6d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertConfig {\n",
              "  \"_name_or_path\": \"./models/distilbert-base-uncased-local\",\n",
              "  \"activation\": \"gelu\",\n",
              "  \"architectures\": [\n",
              "    \"DistilBertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"dim\": 768,\n",
              "  \"dropout\": 0.1,\n",
              "  \"hidden_dim\": 3072,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"distilbert\",\n",
              "  \"n_heads\": 12,\n",
              "  \"n_layers\": 6,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"qa_dropout\": 0.1,\n",
              "  \"seq_classif_dropout\": 0.2,\n",
              "  \"sinusoidal_pos_embds\": false,\n",
              "  \"tie_weights_\": true,\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.21.1\",\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from torch.nn.modules import activation\n",
        "class DistilBertModel(nn.Module):\n",
        "  def __init__(self, train_embedding=False) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = DistilBertForMaskedLM.from_pretrained(\"./models/distilbert-base-uncased-local\", local_files_only=True).to(device)\n",
        "    self.embedding = self.model.get_input_embeddings()\n",
        "    if not train_embedding:\n",
        "      self.embedding.requires_grad_(False)\n",
        "    self.model.set_input_embeddings(nn.Sequential())\n",
        "\n",
        "  def get_config(self):\n",
        "    return self.model.config\n",
        "\n",
        "  def forward(self, x, mask, output_hidden_states=True):\n",
        "    return self.model(x, mask, output_hidden_states=output_hidden_states)\n",
        "\n",
        "class EncoderModel(nn.Module):\n",
        "  def __init__(self, \n",
        "               layer_dim=512, \n",
        "               nhead=8, \n",
        "               activation='gelu',\n",
        "               dropout=0.1,\n",
        "               num_layer=6,\n",
        "               train_embedding=False) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=layer_dim, \n",
        "        nhead=nhead,\n",
        "        dim_feedforward=2048,\n",
        "        activation=activation,\n",
        "        dropout=dropout,\n",
        "        batch_first=False,\n",
        "        norm_first=False,\n",
        "        device=device)\n",
        "    self.model = nn.TransformerEncoder(\n",
        "        encoder_layer, \n",
        "        num_layers=num_layer,\n",
        "        norm=None,\n",
        "        enable_nested_tensor=False)\n",
        "    self.embedding = nn.Embedding(\n",
        "        30522,\n",
        "        layer_dim, \n",
        "        padding_idx=None, \n",
        "        max_norm=None, \n",
        "        norm_type=2.0, \n",
        "        scale_grad_by_freq=False, \n",
        "        sparse=False, \n",
        "        device=device)\n",
        "    \n",
        "    def forward(self, x, mask):\n",
        "      return self.model(x, mask)\n",
        "\n",
        "\n",
        "# model = DistilBertModel(train_embedding=train_embedding)\n",
        "model = EncoderModel(train_embedding=train_embedding)\n",
        "trainer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KNtrwcxDdS13"
      },
      "outputs": [],
      "source": [
        "betas = torch.hstack([torch.zeros(1), torch.linspace(beta_min, beta_max, step_tot)]).to(device)\n",
        "alphas = 1 - betas\n",
        "alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
        "def diffuse_t(x, t):\n",
        "  '''\n",
        "  x_shape: [batch_size, seq_len, dim]\n",
        "  t shape: [sample num]\n",
        "\n",
        "  return shape [batch_size * sample_num, seq_len, dim]\n",
        "  '''\n",
        "  batch_size, seq_len, dim = x.shape\n",
        "  sample_shape = (sample_size, *(1, ) * len(x.shape))\n",
        "\n",
        "  noise = torch.normal(0, 1, x.shape).to(device)\n",
        "  mean = torch.sqrt(alpha_cumprod[t].reshape(sample_shape)) * x \n",
        "  epsilon = noise * torch.sqrt(1 - alpha_cumprod[t]).reshape(sample_shape)\n",
        "  return (mean + epsilon).reshape((sample_size * batch_size, seq_len, dim))\n",
        "\n",
        "def generate_diffuse_pair(x_0, repeat_shape, t, t_next=-1):\n",
        "  '''\n",
        "  x_0 shape: [batch_size, seq_len, dim]\n",
        "  t shape: [sample_num]\n",
        "  repeat shape: (sample_num, 1, 1, ...)\n",
        "  \n",
        "  return (net input, net target)\n",
        "    shape [batch_size * sample_num, seq_len, dim]\n",
        "  '''\n",
        "  if t_next == -1:\n",
        "    # predict x_0\n",
        "    return (diffuse_t(x_0, t), x_0.repeat(repeat_shape))\n",
        "\n",
        "  # predict x_{t_next}\n",
        "  return (diffuse_t(x_0, t), diffuse_t(x_0, t_next))\n",
        "\n",
        "def loss(model, x_input, x_tgt, mask, loss_func):\n",
        "  x_hat = model(x_input, mask, output_hidden_states=True)[1][0]\n",
        "  return loss_func(x_hat, x_tgt)\n",
        "\n",
        "trainer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRU8CjtXdS15"
      },
      "source": [
        "# Define dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6r_AmkWadS15"
      },
      "outputs": [],
      "source": [
        "# define dataset \n",
        "class DPMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, input_df):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_df['text'].tolist()\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # function for batch allocation\n",
        "        texts = []\n",
        "\n",
        "        for b in batch:\n",
        "            texts.append(b)\n",
        "\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "        return {\"input_ids\": encodings[\"input_ids\"].to(device), \"attention_mask\": encodings[\"attention_mask\"].to(device)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"./tokenizers/distilbert-base-uncased-local\", local_files_only=True)\n",
        "train_dataset = DPMDataset(tokenizer, train_df)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Lj9pKtdS16"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GApc9yMOdS16",
        "outputId": "0fdd14f7-3578-4036-c2a1-1a2b31552d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:40<00:00, 10.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 average loss: 0.2694462537765503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:40<00:00, 10.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 average loss: 0.5360242128372192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 average loss: 0.8006168603897095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 average loss: 1.0622565746307373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 average loss: 1.3215457201004028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 average loss: 1.5792444944381714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 average loss: 1.8352642059326172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 average loss: 2.0897789001464844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 average loss: 2.3427443504333496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 419/419 [00:39<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9 average loss: 2.5948922634124756\n"
          ]
        }
      ],
      "source": [
        "# training, \n",
        "model.train()\n",
        "for epoch in range(epoch_num):\n",
        "  acc_loss = 0\n",
        "  for x in tqdm.tqdm(train_loader):\n",
        "    x_0 = model.embedding(x[\"input_ids\"])\n",
        "    repeat_shape = (sample_size, *(1, ) * (len(x_0.shape) - 1))\n",
        "    t = torch.randint(0, step_tot, repeat_shape, device=device)\n",
        "    x_input, x_tgt = generate_diffuse_pair(x_0, repeat_shape, t)\n",
        "\n",
        "    trainer.zero_grad()\n",
        "    l = loss(model, x_input, x_tgt, x[\"attention_mask\"].repeat(repeat_shape), nn.L1Loss())\n",
        "    l.backward()\n",
        "    trainer.step()\n",
        "\n",
        "    acc_loss += l\n",
        "\n",
        "  print(f\"epoch {epoch} average loss: {acc_loss / (step_tot // 30)}\") # TODO: change loss \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piujBB0AjIuB",
        "outputId": "8b266e6a-5d05-4d92-e0e9-3037d221d52e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 98])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[\"attention_mask\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoVoUgi4dS17",
        "outputId": "c9580a2e-a6c2-4d9c-fb47-ad64811ec497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noise added\n",
            "inference finished\n",
            "origin text:  Critics have even taken to dobbing in Katrina Bungard to National Party leader Bill English when they see her sign-written car bearing her name and photo parked in disabled parks .\n",
            "inferred:  .................. -... ~... - ~............ -......................... ~............ ~............... -....\n"
          ]
        }
      ],
      "source": [
        "# trial on inference\n",
        "for text in train_loader:\n",
        "  break\n",
        "repeat_shape = (sample_size, *(1, ) * (len(x_0.shape) - 1))\n",
        "t = torch.randint(0, step_tot, repeat_shape, device=device)\n",
        "noised_text = diffuse_t(model.embedding(text[\"input_ids\"]), t)\n",
        "print(\"noise added\")\n",
        "restored = model(noised_text, text[\"attention_mask\"].repeat(repeat_shape), output_hidden_states=True)\n",
        "print(\"inference finished\")\n",
        "print(\"origin text: \", train_df.loc[0][\"text\"])\n",
        "print(\"inferred: \", tokenizer.decode(torch.softmax(restored[0][0], dim=-1).argmax(dim=-1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uFbo4aynOz7",
        "outputId": "317bdcda-9d18-427c-c355-fe98a798f29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  4401,  2031,  ...,     0,     0,     0],\n",
              "         [  101, 13573,  1998,  ...,     0,     0,     0],\n",
              "         [  101,  2720,  8716,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  1000,  2012,  ...,     0,     0,     0],\n",
              "         [  101,  1996, 20871,  ...,     0,     0,     0],\n",
              "         [  101,  1000,  1000,  ...,     0,     0,     0]], device='cuda:0'),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H61Lo_yNdS17"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save({\"net\": model.to(torch.device(\"cpu\"))}, \"model.pickle\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l-nO5MNtdS1v",
        "NRU8CjtXdS15",
        "65Lj9pKtdS16"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('mlenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}