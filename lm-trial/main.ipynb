{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-nO5MNtdS1v"
      },
      "source": [
        "# Get preprocessed 'DontPatronizeMe' dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XS1i5FqdS1x",
        "outputId": "991bdbb8-5f10-4fa5-db41-8d26deaf1ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-08-19 15:04:57--  https://drive.google.com/uc?export=download&id=1-gNxTZfDL0aOpzOnxE80M29dUVjSoozn\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.100, 173.194.203.138, 173.194.203.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... ^C\n",
            "--2022-08-19 15:04:58--  https://drive.google.com/uc?export=download&id=1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.100, 173.194.203.138, 173.194.203.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-10-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j1lhliqq1hhsvanps0a2iu24hvtg09f7/1660921425000/09836793732558009118/*/1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh?e=download&uuid=639aab9b-351c-4fdf-b62c-61c6ea811213 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-08-19 15:04:59--  https://doc-10-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j1lhliqq1hhsvanps0a2iu24hvtg09f7/1660921425000/09836793732558009118/*/1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh?e=download&uuid=639aab9b-351c-4fdf-b62c-61c6ea811213\n",
            "Resolving doc-10-9s-docs.googleusercontent.com (doc-10-9s-docs.googleusercontent.com)... 74.125.199.132, 2607:f8b0:400e:c02::84\n",
            "Connecting to doc-10-9s-docs.googleusercontent.com (doc-10-9s-docs.googleusercontent.com)|74.125.199.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 483768 (472K) [text/csv]\n",
            "Saving to: ‘valid.csv’\n",
            "\n",
            "valid.csv           100%[===================>] 472.43K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-08-19 15:04:59 (116 MB/s) - ‘valid.csv’ saved [483768/483768]\n",
            "\n",
            "--2022-08-19 15:04:59--  https://drive.google.com/uc?export=download&id=1-13l35-18IYPFSV_36llsJbb7c4Gu2o0\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.203.100, 173.194.203.138, 173.194.203.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.203.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-00-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nkib5pt8p4l4grhsch447heqd5k9dcoo/1660921500000/09836793732558009118/*/1-13l35-18IYPFSV_36llsJbb7c4Gu2o0?e=download&uuid=b9688887-59d3-4677-8a68-b8e2f26defbe [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-08-19 15:05:00--  https://doc-00-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/nkib5pt8p4l4grhsch447heqd5k9dcoo/1660921500000/09836793732558009118/*/1-13l35-18IYPFSV_36llsJbb7c4Gu2o0?e=download&uuid=b9688887-59d3-4677-8a68-b8e2f26defbe\n",
            "Resolving doc-00-9s-docs.googleusercontent.com (doc-00-9s-docs.googleusercontent.com)... 74.125.199.132, 2607:f8b0:400e:c02::84\n",
            "Connecting to doc-00-9s-docs.googleusercontent.com (doc-00-9s-docs.googleusercontent.com)|74.125.199.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 602497 (588K) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>] 588.38K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-08-19 15:05:00 (124 MB/s) - ‘test.csv’ saved [602497/602497]\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 9, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main_parser.py\", line 8, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/cmdoptions.py\", line 23, in <module>\n",
            "    from pip._internal.cli.parser import ConfigOptionParser\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/parser.py\", line 12, in <module>\n",
            "    from pip._internal.configuration import Configuration, ConfigurationError\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/configuration.py\", line 21, in <module>\n",
            "    from pip._internal.exceptions import (\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/exceptions.py\", line 7, in <module>\n",
            "    from pip._vendor.pkg_resources import Distribution\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 83, in <module>\n",
            "    __import__('pip._vendor.packaging.specifiers')\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 963, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 906, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1280, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1252, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1364, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 81, in _path_stat\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-gNxTZfDL0aOpzOnxE80M29dUVjSoozn' -O 'train.csv'\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-cSiEWP_NbDu7fo_7s8O5P163oKLQcBh' -O 'valid.csv'\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1-13l35-18IYPFSV_36llsJbb7c4Gu2o0' -O 'test.csv'\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH5doKeWoxgJ"
      },
      "source": [
        "# Import package, model, dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "U6xCt976dS1z",
        "outputId": "090d8988-39aa-4738-b185-37cd2d0f2717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device:  cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "from transformers import (\n",
        "    DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
        "    BertTokenizer, BertModel as Bert\n",
        ")\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  dev = \"cuda:0\"\n",
        "else:\n",
        "  dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print(\"using device: \", dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SbAapRyQdS10"
      },
      "outputs": [],
      "source": [
        "# read pandas data\n",
        "train_path = './train.csv'\n",
        "valid_path = './valid.csv'\n",
        "test_path = './test.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path).dropna()\n",
        "valid_df = pd.read_csv(valid_path).dropna()\n",
        "test_df = pd.read_csv(test_path).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "1Dgq6KK1dS10"
      },
      "outputs": [],
      "source": [
        "# download pretrained model and tokenizer\n",
        "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
        "  tokenizer = tokenizer_class.from_pretrained(name)\n",
        "  tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
        "  model = model_class.from_pretrained(name)\n",
        "  model.save_pretrained(f\"./models/{name}-local/\")\n",
        "\n",
        "save_model_tokenizer(DistilBertTokenizer, DistilBertForMaskedLM, \"distilbert-base-uncased\")\n",
        "# save_model_tokenizer(BertTokenizer, Bert, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drw9XZ_GdS11"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "chPgGq3bdS12"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 16\n",
        "max_length = 128 # max text length\n",
        "learning_rate = 1e-4\n",
        "epoch_num = 4\n",
        "linear_probe = False\n",
        "\n",
        "# diffusion hyperparameter\n",
        "beta_min = 0.0001\n",
        "beta_max = 0.02\n",
        "step_tot = 2000 # total noise adding steps\n",
        "sample_size = 3 # number of sample steps in each diffuse sequence\n",
        "x_0_prediction = False # if model predicts x_0 or x_{t-1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZwRatWVdS13"
      },
      "source": [
        "# Model, trainer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA6YHlL7jIt-",
        "outputId": "cc634fb2-8429-4086-aba3-cf311f66b6d8"
      },
      "outputs": [],
      "source": [
        "class DistilBertModel(nn.Module):\n",
        "  def __init__(self, config=None) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = DistilBertForMaskedLM.from_pretrained(\"./models/distilbert-base-uncased-local\", local_files_only=True, config=config).to(device)\n",
        "    \n",
        "    self.embedding = copy.deepcopy(self.model.get_input_embeddings().requires_grad_(False))\n",
        "    self.projection = copy.deepcopy(self.model.get_output_embeddings().requires_grad_(False))\n",
        "    self.model.set_input_embeddings(nn.Sequential())\n",
        "    self.model.set_output_embeddings(nn.Sequential())\n",
        "\n",
        "    # print(self.model.config)\n",
        "\n",
        "  def parameters(self):\n",
        "    return list(model.model.parameters()) + list(model.embedding.parameters()) + list(model.projection.parameters())\n",
        "  \n",
        "  def forward(self, x, mask):\n",
        "    '''\n",
        "    return \n",
        "      feature_out, shape: [batch_size, seq_len, dim]\n",
        "      vocab_out, shape: [batch_size, seq_len, vocab_size]\n",
        "    '''\n",
        "    \n",
        "    x_out = self.model(x, mask)[0]\n",
        "    return self.projection(x_out), x_out\n",
        "\n",
        "class EncoderModel(nn.Module): # ABANDONED: mask shape not known meaning\n",
        "  def __init__(self, \n",
        "               layer_dim=512, \n",
        "               nhead=8, \n",
        "               activation='gelu',\n",
        "               dropout=0.1,\n",
        "               num_layer=6,\n",
        "               train_embedding=False) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=layer_dim, \n",
        "        nhead=nhead,\n",
        "        dim_feedforward=2048,\n",
        "        activation=activation,\n",
        "        dropout=dropout,\n",
        "        batch_first=False,\n",
        "        norm_first=False,\n",
        "        device=device)\n",
        "    self.model = nn.TransformerEncoder(\n",
        "        encoder_layer, \n",
        "        num_layers=num_layer,\n",
        "        norm=None)\n",
        "    self.embedding = nn.Embedding(\n",
        "        30522,\n",
        "        layer_dim, \n",
        "        padding_idx=None, \n",
        "        max_norm=None, \n",
        "        norm_type=2.0, \n",
        "        scale_grad_by_freq=False, \n",
        "        sparse=False, \n",
        "        device=device)\n",
        "    if not train_embedding:\n",
        "      self.embedding.requires_grad_(False)\n",
        "    \n",
        "  def forward(self, x, mask):\n",
        "    return self.model(x, mask)\n",
        "\n",
        "class BertModel(nn.Module): # ABANDONED\n",
        "  def __init__(self, train_embedding=False) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = Bert.from_pretrained(\"./models/bert-base-cased-local/\", local_files_only=True)\n",
        "\n",
        "    self.embedding = self.model.get_input_embeddings()\n",
        "    if not train_embedding:\n",
        "      self.embedding.requires_grad_(False)\n",
        "    self.model.set_input_embeddings(nn.Sequential())\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    return self.model(x, mask)\n",
        "\n",
        "configuration = DistilBertConfig()\n",
        "model = DistilBertModel(config=configuration)\n",
        "# model = EncoderModel(train_embedding=train_embedding)\n",
        "# model = BertModel(train_embedding=train_embedding)\n",
        "\n",
        "if linear_probe:  \n",
        "  # TODO: linear probation not supported\n",
        "  NotImplementedError()\n",
        "  # trainer = optim.Adam(model.projection.parameters(), lr=learning_rate)\n",
        "else:\n",
        "  # parameter only include model, no embedding layer\n",
        "  trainer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KNtrwcxDdS13"
      },
      "outputs": [],
      "source": [
        "betas = torch.hstack([torch.zeros(1), torch.linspace(beta_min, beta_max, step_tot)]).to(device)\n",
        "alphas = 1 - betas\n",
        "alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
        "def diffuse_t(x, t):\n",
        "  '''\n",
        "  x_shape: [batch_size, seq_len, dim]\n",
        "  t shape: [sample num]\n",
        "\n",
        "  return shape [batch_size * sample_num, seq_len, dim]\n",
        "  '''\n",
        "  batch_size, seq_len, dim = x.shape\n",
        "  sample_shape = (sample_size, *(1, ) * len(x.shape))\n",
        "\n",
        "  noise = torch.normal(0, 1, x.shape).to(device)\n",
        "  mean = torch.sqrt(alpha_cumprod[t].reshape(sample_shape)) * x \n",
        "  epsilon = noise * torch.sqrt(1 - alpha_cumprod[t]).reshape(sample_shape)\n",
        "  return (mean + epsilon).reshape((sample_size * batch_size, seq_len, dim))\n",
        "\n",
        "def generate_diffuse_pair(x_0, repeat_shape, t, t_next=-1):\n",
        "  '''\n",
        "  x_0 shape: [batch_size, seq_len, dim]\n",
        "  t shape: [sample_num]\n",
        "  repeat shape: (sample_num, 1, 1, ...)\n",
        "  \n",
        "  return (net input, net target)\n",
        "    shape [batch_size * sample_num, seq_len, dim]\n",
        "  '''\n",
        "  if t_next == -1:\n",
        "    # predict x_0\n",
        "    return (diffuse_t(x_0, t), x_0.repeat(repeat_shape))\n",
        "\n",
        "  # predict x_{t_next}\n",
        "  return (diffuse_t(x_0, t), diffuse_t(x_0, t_next))\n",
        "\n",
        "def loss(model, x_input, x_tgt, mask, loss_func):\n",
        "  _, x_hat = model(x_input, mask)\n",
        "  return loss_func(x_hat, x_tgt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRU8CjtXdS15"
      },
      "source": [
        "# Define dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6r_AmkWadS15"
      },
      "outputs": [],
      "source": [
        "# define dataset \n",
        "class DPMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, input_df):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_df['text'].tolist()\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # function for batch allocation\n",
        "        texts = []\n",
        "\n",
        "        for b in batch:\n",
        "            texts.append(b)\n",
        "\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "        return {\"input_ids\": encodings[\"input_ids\"].to(device), \"attention_mask\": encodings[\"attention_mask\"].to(device)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"./tokenizers/distilbert-base-uncased-local/\", local_files_only=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"./tokenizers/bert-base-cased-local\", local_files_only=True)\n",
        "\n",
        "train_dataset = DPMDataset(tokenizer, train_df)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Lj9pKtdS16"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GApc9yMOdS16",
        "outputId": "0fdd14f7-3578-4036-c2a1-1a2b31552d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/419 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([48, 120, 768]) torch.Size([48, 120, 768])\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "model.train()\n",
        "print(\"start training\")\n",
        "for epoch in range(epoch_num):\n",
        "  acc_loss = 0\n",
        "  with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch: \n",
        "    for epoch, x in enumerate(tepoch):\n",
        "      x_0 = model.embedding(x[\"input_ids\"])\n",
        "      repeat_shape = (sample_size, *(1, ) * (len(x_0.shape) - 1))\n",
        "      t = torch.randint(0, step_tot, repeat_shape, device=device)\n",
        "      if x_0_prediction:\n",
        "        x_input, x_tgt = generate_diffuse_pair(x_0, repeat_shape, t)\n",
        "      else:\n",
        "        x_input, x_tgt = generate_diffuse_pair(x_0, repeat_shape, t, torch.max(t - 30, 0))\n",
        "\n",
        "      trainer.zero_grad()\n",
        "      l = loss(model, x_input, x_tgt, x[\"attention_mask\"].repeat(repeat_shape), nn.L1Loss())\n",
        "      l.backward()\n",
        "      trainer.step()\n",
        "\n",
        "      acc_loss += l\n",
        "\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "      tepoch.set_postfix(Loss=l)\n",
        "\n",
        "  print(f\"epoch {epoch} average loss: {acc_loss / len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoVoUgi4dS17",
        "outputId": "c9580a2e-a6c2-4d9c-fb47-ad64811ec497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "noise added\n",
            "origin text:  Critics have even taken to dobbing in Katrina Bungard to National Party leader Bill English when they see her sign-written car bearing her name and photo parked in disabled parks .\n",
            "inferred:  [CLS] critics have even taken to dobbing in katrina bungard to national party leader bill english when they see her sign - written car bearing her name and photo parked in disabled parks. [SEP]\n",
            "torch.Size([3, 37, 768]) torch.Size([3, 37, 768])\n",
            "loss tensor(0.0423, grad_fn=<L1LossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# trial on inference\n",
        "model = torch.load(\"model.pickle\")[\"net\"]\n",
        "text = tokenizer(train_df.loc[0][\"text\"], return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n",
        "x_0 = model.embedding(text[\"input_ids\"])\n",
        "repeat_shape = (sample_size, *(1, ) * (len(x_0.shape) - 1))\n",
        "t = torch.randint(0, step_tot, repeat_shape, device=device)\n",
        "noised_text = diffuse_t(x_0, t)\n",
        "print(\"noise added\")\n",
        "\n",
        "restored = noised_text\n",
        "for i in range(10):\n",
        "  out, restored = model(restored, text[\"attention_mask\"].repeat(repeat_shape)) \n",
        "\n",
        "\n",
        "print(\"origin text: \", train_df.loc[0][\"text\"])\n",
        "# print(\"inferred: \", tokenizer.decode(torch.softmax(out, dim=-1).argmax(dim=-1)[0]))\n",
        "print(\"inferred: \", tokenizer.decode((x_0 @ model.embedding.weight.data.T).argmax(dim=-1)[0]))\n",
        "print(\"loss\", loss(model, noised_text, x_0.repeat(repeat_shape), text[\"attention_mask\"].repeat(repeat_shape), nn.L1Loss()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H61Lo_yNdS17"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save({\"net\": model.to(torch.device(\"cpu\"))}, \"model.pickle\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l-nO5MNtdS1v",
        "NRU8CjtXdS15",
        "65Lj9pKtdS16"
      ],
      "name": "main.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('mlenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
