{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://storage.googleapis.com/kaggle-data-sets/771078/1328792/compressed/flickr8k.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220828%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220828T123221Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2da96ff4d583711c087af4482eb38f93034c0c9d78e743b4a3520b6480ef3d25998461288db8e5c23b18ad9270bec9d16d68ddd07aa8a20886f26f69f65afdb17b191cebda77a46b8d7fa9f78ab59359d935b544f1a434cfe472b943ed4293d74a1f821b4ca04c80d745791d9cafceb395cfe81006007afc9ab5f292ebb29954b1c7df69c47b556f1f71fef89ddfb924802df066f704916b81dfa36f978f0d70ebe0d71012b7c1e1d4937b3ce36520bfb71a818f449395297015e2dfec6ed2f29fb2b652469134a90fa4143e5095f77645632c6ebe4814fff501569badecab290d50d8316b3ea48667fdb7ab9f9fc86666e07026bf1470b41a0151de1b0e0323'\n",
    "!unzip -q flickr8k.zip -d flickr8k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import (\n",
    "  DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
    "  CLIPProcessor, CLIPModel as CLIP, CLIPConfig\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"using device: \", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and tokenizer\n",
    "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
    "  if tokenizer_class is not None:\n",
    "    tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
    "  if model_class is not None:\n",
    "    model = model_class.from_pretrained(name)\n",
    "    model.save_pretrained(f\"./models/{name}-local/\")\n",
    "\n",
    "save_model_tokenizer(CLIPProcessor, CLIP, \"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 128 # max text length\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCH_NUM = 1\n",
    "ROUNDING_WEIGHT = 0.3 # weight of rounding term, the probability of regenerated sequence \n",
    "\n",
    "# diffusion hyperparameter\n",
    "BETA_MIN = 0.0001\n",
    "BETA_MAX = 0.02\n",
    "STEP_TOT = 2000 # total noise adding steps\n",
    "COSIN_SCHEDULE = True # if alpha sequence is scheduled in cosin instead of linear patten\n",
    "SAMPLE_SIZE = 3 # number of sample steps in each diffuse sequence\n",
    "X_0_PREDICTION = True # if model predicts x_0 or x_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertModel(nn.Module):\n",
    "  def __init__(self, projection, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      projection: torch.tensor\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "\n",
    "    self.projection = nn.Linear(projection.shape[-2], projection.shape[-1], device=device)\n",
    "    self.projection.weight.data = projection\n",
    "    self.projection.bias.data = torch.zeros(self.projection.bias.data.shape, device=device)\n",
    "    self.projection.requires_grad_(False)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.model.parameters()\n",
    "  \n",
    "  def forward(self, x, mask):\n",
    "    '''\n",
    "    return \n",
    "      feature_out, shape: [batch_size, seq_len, dim]\n",
    "      vocab_out, shape: [batch_size, seq_len, vocab_size]\n",
    "    '''\n",
    "    \n",
    "    x_out = self.model(x, mask)[0]\n",
    "    return self.projection(x_out), x_out\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"./tokenizers/openai/clip-vit-base-patch32-local\")\n",
    "clip = CLIP.from_pretrained(\"./models/openai/clip-vit-base-patch32-local\")\n",
    "\n",
    "configuration = DistilBertConfig(vocab_size=clip_processor.tokenizer.vocab_size, dim=clip.projection_dim, n_heads=8)\n",
    "model = DistilBertModel(clip.get_submodule(\"text_model.embeddings.token_embedding\").weight.data, config=configuration)\n",
    "\n",
    "# parameter only include model, no embedding layer\n",
    "# trainer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kCLIPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, dir, clip_processor, clip) -> None:\n",
    "    self.dir = dir\n",
    "    self.caption = pd.read_csv(f\"{dir}/captions.txt\")\n",
    "\n",
    "    self.clip = clip\n",
    "    self.clip_processor = clip_processor\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for b in batch:\n",
    "      images.append(Image.open(f\"{self.dir}/images/{b['image']}\"))\n",
    "      captions.append(b[\"caption\"])\n",
    "\n",
    "    inputs = self.clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True)\n",
    "    outputs = self.clip(**inputs)\n",
    "\n",
    "    return outputs.text_embeds, outputs.image_embeds\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.caption)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.caption.loc[idx]\n",
    "\n",
    "# if dict on clip (4.5 s/batch 3.5h), dict on image (4.7 s/batch 3.5h), load on request (4.5 ) better\n",
    "# try on load on request in colect func\n",
    "train_dataset = Flickr8kCLIPDataset(\"flickr8k/\", clip_processor, clip)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/633 [01:07<3:54:58, 22.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=2'>3</a>\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_loader) \u001b[39mas\u001b[39;00m process:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=4'>5</a>\u001b[0m   \u001b[39mfor\u001b[39;00m image, text \u001b[39min\u001b[39;00m process:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=5'>6</a>\u001b[0m     image_all \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvstack([image_all, image])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=6'>7</a>\u001b[0m     text_all \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mvstack([text_all, text])\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 12\u001b[0m in \u001b[0;36mFlickr8kCLIPDataset.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=13'>14</a>\u001b[0m   captions\u001b[39m.\u001b[39mappend(b[\u001b[39m\"\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=15'>16</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_processor(text\u001b[39m=\u001b[39mcaptions, images\u001b[39m=\u001b[39mimages, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000028?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\u001b[39m.\u001b[39mtext_embeds, outputs\u001b[39m.\u001b[39mimage_embeds\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1024\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1024\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m   1025\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1031\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\n\u001b[1;32m   1032\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1033\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1038\u001b[0m )\n\u001b[1;32m   1040\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:774\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    772\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 774\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    775\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    776\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    777\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    778\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    782\u001b[0m pooled_output \u001b[39m=\u001b[39m last_hidden_state[:, \u001b[39m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:578\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    572\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    573\u001b[0m         hidden_states,\n\u001b[1;32m    574\u001b[0m         attention_mask,\n\u001b[1;32m    575\u001b[0m         causal_attention_mask,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m    580\u001b[0m         attention_mask,\n\u001b[1;32m    581\u001b[0m         causal_attention_mask,\n\u001b[1;32m    582\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    585\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:321\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    318\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    320\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 321\u001b[0m hidden_states, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    322\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    323\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    324\u001b[0m     causal_attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    325\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    327\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    329\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:211\u001b[0m, in \u001b[0;36mCLIPAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m# get query proj\u001b[39;00m\n\u001b[1;32m    210\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(hidden_states) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m--> 211\u001b[0m key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[1;32m    212\u001b[0m value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[1;32m    214\u001b[0m proj_shape \u001b[39m=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_all = torch.tensor([]).reshape((0, 512))\n",
    "text_all = torch.tensor([]).reshape((0, 512))\n",
    "cnt = 0\n",
    "with tqdm.tqdm(train_loader) as process:\n",
    "  for image, text in process:\n",
    "    image_all = torch.vstack([image_all, image])\n",
    "    text_all = torch.vstack([text_all, text])\n",
    "    cnt += 1\n",
    "    if cnt > 10:\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(image_all, \"image_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = torch.utils.data.TensorDataset(image_all, text_all)\n",
    "small_dataloader = DataLoader(small_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 527.91it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm.tqdm(small_dataloader) as process:\n",
    "  for image, text in process:\n",
    "    continue\n",
    "    # image_all = torch.vstack([image_all, image])\n",
    "    # text_all = torch.vstack([text_all, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dict = torch.load(\"image_dict.pickle\")\n",
    "for k in image_dict:\n",
    "  break\n",
    "image_dict[k][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TODO: COCO dataset'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TODO: COCO dataset'''\n",
    "\n",
    "# import torchvision.transforms as transforms\n",
    "# cap = CocoCaptions(root = 'dir where images are',\n",
    "#                         annFile = 'json annotation file',\n",
    "#                         transform=transforms.PILToTensor())\n",
    "\n",
    "# print('Number of samples: ', len(cap))\n",
    "# img, target = cap[3] # load 4th sample\n",
    "\n",
    "# print(\"Image Size: \", img.size())\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512]) torch.Size([1, 512]) torch.Size([4, 7, 512]) torch.Size([4, 512]) torch.Size([1, 50, 768]) torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0148,  0.0070, -0.0234,  ..., -0.0508, -0.0438,  0.0033],\n",
       "         [-0.0057,  0.0125,  0.0188,  ..., -0.0266,  0.0032, -0.0344],\n",
       "         [-0.0087,  0.0342, -0.0107,  ..., -0.0764, -0.0228, -0.0132],\n",
       "         [-0.0107,  0.0212, -0.0263,  ..., -0.0634, -0.0256,  0.0114]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[ 1.2610e-02,  4.0371e-02, -2.9072e-02,  1.7522e-02, -1.5079e-02,\n",
       "           7.2654e-03,  1.9884e-02, -2.3690e-02, -2.1255e-02,  2.8416e-02,\n",
       "           5.0445e-03, -1.7513e-02,  5.5213e-02, -3.2175e-02, -3.5756e-02,\n",
       "           1.6965e-02, -1.6372e-02, -7.2888e-03, -2.5646e-02, -1.0919e-02,\n",
       "          -2.9222e-02, -1.1313e-03, -5.3982e-03, -9.1862e-02,  2.0838e-02,\n",
       "           5.8341e-02, -4.7764e-03, -1.1011e-02,  4.1024e-02,  1.5600e-02,\n",
       "          -2.1152e-02,  2.1732e-02, -1.1583e-02,  8.4909e-03,  4.8979e-02,\n",
       "          -1.6090e-02,  2.4998e-02, -1.1805e-02,  1.8637e-02, -2.0716e-02,\n",
       "          -5.0996e-02, -1.2215e-02, -6.0782e-02, -5.3689e-02, -1.1481e-05,\n",
       "          -1.3117e-01,  4.1282e-02,  3.5481e-03, -5.9600e-02,  2.4972e-02,\n",
       "           1.8531e-02, -4.3098e-03,  6.2209e-02,  1.0824e-02, -2.8156e-03,\n",
       "           1.3805e-02,  1.5621e-02, -1.8775e-03, -4.3966e-02,  9.9269e-03,\n",
       "           1.3632e-02, -2.0809e-02,  2.7368e-02, -1.5131e-02,  5.1952e-02,\n",
       "          -4.1557e-02,  2.9136e-02,  1.2766e-01,  2.2585e-02,  6.8574e-05,\n",
       "           5.7141e-03, -2.6905e-02, -1.5948e-03, -1.7834e-02,  2.5276e-02,\n",
       "          -2.8798e-03,  1.1817e-02, -2.7546e-03,  1.4623e-03,  3.7824e-02,\n",
       "           3.7483e-02,  6.0092e-02,  7.9069e-02,  3.9702e-02,  1.0519e-01,\n",
       "           3.0208e-02,  5.9905e-02, -3.4391e-03,  1.6947e-02, -2.9417e-02,\n",
       "           3.0197e-02,  3.0297e-02, -6.3492e-01,  1.9393e-02,  5.4832e-03,\n",
       "          -2.4611e-02,  3.8779e-03,  1.8236e-02, -4.3788e-02, -1.2133e-01,\n",
       "           2.2295e-02,  8.0265e-03, -2.2479e-02,  5.5633e-03, -1.6173e-03,\n",
       "           3.6341e-02, -1.3210e-01, -3.2626e-02, -1.1473e-02, -1.6138e-02,\n",
       "           2.1649e-02, -3.9016e-02,  2.3367e-03,  1.0400e-02, -3.0831e-02,\n",
       "          -1.8263e-02,  2.9457e-02,  4.1970e-03,  2.3986e-02,  2.4490e-02,\n",
       "           1.6749e-03,  5.0654e-02, -1.7751e-02,  3.0274e-03,  2.0949e-02,\n",
       "           7.7168e-03,  2.6743e-02, -5.0658e-03,  8.6433e-03,  2.0810e-03,\n",
       "           3.3127e-02,  7.4537e-03,  1.2009e-02,  8.4111e-02,  2.8137e-02,\n",
       "          -9.1459e-04, -1.0853e-02,  3.0864e-02, -2.2108e-02, -2.1687e-02,\n",
       "          -5.9415e-03, -3.7160e-02, -1.8000e-02,  3.6157e-02,  9.7966e-03,\n",
       "           2.1710e-04, -1.3175e-02,  6.3973e-02, -3.4976e-02,  1.1588e-02,\n",
       "          -2.0675e-02,  3.3247e-02,  6.6736e-02, -3.2601e-02, -3.9195e-02,\n",
       "          -3.4965e-02,  5.1602e-03, -3.3214e-02,  3.9873e-03,  7.2989e-03,\n",
       "          -1.5455e-02,  3.3883e-03, -8.9051e-03,  4.9818e-02, -4.8453e-03,\n",
       "           2.6886e-02,  2.1966e-02, -1.7461e-02, -7.9770e-04, -3.4328e-02,\n",
       "           3.0941e-02,  4.7258e-02, -1.2488e-02, -5.9576e-02,  9.6150e-04,\n",
       "          -8.5209e-02, -1.8688e-02, -1.4637e-02,  2.4912e-02,  2.2696e-02,\n",
       "           9.1335e-03,  2.1876e-03,  8.3690e-03,  2.3509e-02, -2.9227e-02,\n",
       "           1.6468e-02,  1.1648e-02, -2.5692e-02, -2.0719e-02,  2.3438e-02,\n",
       "          -1.7986e-02,  1.8366e-02,  1.4616e-02, -4.7310e-02, -3.6057e-02,\n",
       "           6.5018e-03,  1.8981e-02, -1.8698e-02,  8.5233e-02,  1.2481e-03,\n",
       "          -5.5767e-03, -2.4612e-02, -2.3549e-02,  5.3210e-02,  3.6028e-02,\n",
       "          -1.6289e-02, -4.1128e-03, -1.4921e-03,  1.3712e-03,  4.4036e-02,\n",
       "          -3.7534e-02, -5.4104e-02,  2.4408e-02, -2.3447e-03,  2.9982e-02,\n",
       "          -2.3795e-02,  1.4147e-02, -1.3537e-02, -4.9357e-02, -1.1201e-02,\n",
       "           5.1251e-02,  3.7526e-02,  1.1817e-03, -1.7909e-03, -5.2823e-02,\n",
       "          -2.6804e-02,  1.3339e-02,  5.0894e-03,  3.7423e-03,  4.0152e-02,\n",
       "          -2.0600e-02, -1.3252e-02, -3.0142e-02,  3.4862e-04,  3.1722e-02,\n",
       "          -7.9634e-02,  1.4260e-02, -1.8743e-03,  3.5086e-02, -1.3714e-02,\n",
       "          -1.9581e-03, -8.6642e-03, -1.6612e-02, -3.3081e-02, -1.5692e-02,\n",
       "          -1.7971e-02, -2.4824e-02,  7.7487e-03, -1.7018e-02, -7.6224e-03,\n",
       "           7.9110e-03,  9.2628e-03, -9.7196e-03, -1.9090e-02, -2.0533e-02,\n",
       "           4.5045e-03, -9.6240e-03,  8.5126e-03,  2.1224e-02,  3.4655e-02,\n",
       "          -2.6863e-02,  2.2102e-02,  4.5115e-02, -3.0984e-02,  2.1995e-02,\n",
       "          -5.9068e-02, -3.4115e-02,  2.2000e-02, -6.1139e-03, -1.8768e-02,\n",
       "           2.0100e-02, -8.9858e-03,  2.8241e-02, -4.4595e-02,  1.5449e-03,\n",
       "           2.2246e-02, -5.4105e-02, -5.9798e-02, -1.7682e-02, -1.4521e-02,\n",
       "           1.4140e-02,  3.0436e-03, -3.0486e-02, -3.5736e-03, -2.0570e-02,\n",
       "           3.2590e-02,  5.0886e-02,  3.5596e-02,  2.4266e-03, -1.4969e-02,\n",
       "          -2.3736e-02, -8.2492e-03, -1.1251e-02, -2.6573e-02,  2.9343e-03,\n",
       "          -9.1168e-03, -3.9659e-02, -2.7572e-02, -9.8296e-03, -1.6505e-03,\n",
       "           2.8600e-02,  2.5146e-02, -2.3048e-03,  1.6665e-02,  1.9419e-02,\n",
       "          -4.3316e-02,  1.3601e-02,  4.9559e-02,  1.9474e-02,  1.3701e-03,\n",
       "          -1.6992e-02,  1.5064e-02,  8.4024e-02, -5.3612e-03,  7.1499e-02,\n",
       "          -6.9853e-03, -3.0340e-02,  4.3515e-02,  1.8944e-02, -1.0724e-01,\n",
       "           5.6068e-02, -3.1196e-02, -5.8067e-02, -1.1762e-02,  3.1733e-02,\n",
       "           1.0398e-02, -2.0249e-02, -4.5951e-02,  6.4921e-03, -1.6498e-02,\n",
       "          -1.1364e-03, -1.1065e-02, -1.3862e-02, -3.4145e-02, -3.8677e-02,\n",
       "           2.2100e-03, -4.0730e-02, -1.3510e-02,  3.2772e-02, -3.3708e-02,\n",
       "           1.2732e-03, -5.7398e-02,  1.5657e-02,  1.2940e-02, -2.0393e-03,\n",
       "           1.6887e-02, -3.2525e-02, -3.4938e-02,  1.9912e-02,  2.7916e-02,\n",
       "          -4.4904e-03,  5.1065e-03,  6.2902e-02, -3.7583e-02,  3.5137e-02,\n",
       "          -2.0950e-03, -1.1589e-02, -8.2777e-02,  3.1062e-02, -3.2251e-02,\n",
       "           3.0473e-02, -5.5434e-02, -2.2270e-02,  2.4216e-02,  5.5674e-02,\n",
       "          -3.2092e-02,  5.4332e-03, -9.3366e-02, -4.8100e-02,  2.9811e-02,\n",
       "           2.9217e-02,  1.0263e-02, -1.1122e-03,  1.7588e-02,  5.7449e-03,\n",
       "          -2.3991e-03,  1.0035e-01,  4.3891e-02,  4.9514e-02,  3.2403e-02,\n",
       "           7.5471e-03,  2.7458e-02, -1.5806e-02, -5.2774e-02,  1.5506e-02,\n",
       "           8.5520e-02,  4.1627e-02,  6.0711e-02, -3.0616e-02,  5.1851e-02,\n",
       "          -2.7358e-02, -4.2612e-02, -4.3986e-03,  1.6483e-02,  6.2677e-03,\n",
       "          -5.9561e-03,  9.4748e-02,  1.3007e-02,  1.4370e-02, -6.5182e-02,\n",
       "           3.3722e-02, -1.7433e-02,  1.1792e-01, -4.0764e-02,  3.5096e-02,\n",
       "          -3.8642e-02,  1.1893e-02, -5.0852e-02,  1.1484e-02, -3.0997e-02,\n",
       "          -3.6127e-03,  3.0553e-02, -4.6826e-02,  9.9134e-03, -2.8970e-02,\n",
       "           4.7078e-02, -4.0767e-02, -1.7378e-02,  8.2992e-03, -2.9276e-02,\n",
       "           2.0415e-02, -2.4546e-02,  3.8291e-02, -1.9621e-02,  2.9666e-02,\n",
       "          -1.4637e-02, -4.5986e-02,  1.6892e-02,  3.7158e-03, -4.8347e-03,\n",
       "           3.1246e-02,  6.0346e-02, -8.2057e-02,  3.1580e-02, -2.7134e-02,\n",
       "           7.2853e-03,  1.5881e-02,  6.3880e-04, -1.4105e-03,  5.7335e-02,\n",
       "           3.3124e-02,  5.0374e-02,  3.8593e-02,  3.1531e-02,  3.3529e-02,\n",
       "          -4.7504e-02,  2.4452e-02, -1.7577e-02, -8.6430e-03,  2.4560e-02,\n",
       "          -2.6202e-02,  3.3179e-02, -7.6041e-02,  9.4768e-03,  1.0165e-02,\n",
       "          -1.4737e-02,  4.0772e-02, -6.0771e-02, -3.6198e-02, -1.2800e-02,\n",
       "           4.7378e-03,  4.5666e-03, -4.4599e-02,  2.3779e-04, -5.9890e-03,\n",
       "          -2.6861e-02,  7.4794e-04, -1.4558e-02,  7.1895e-03, -6.0779e-03,\n",
       "          -4.5211e-02,  1.6375e-02,  2.3669e-02, -2.4130e-02,  2.7951e-02,\n",
       "           2.7808e-02, -1.2122e-02, -1.8557e-02, -3.9877e-02, -3.2039e-02,\n",
       "          -3.3675e-02, -1.3554e-02,  1.2750e-03, -2.6564e-03, -1.8499e-02,\n",
       "           1.4994e-02, -4.2411e-02,  4.8535e-02, -1.6597e-02,  2.5517e-02,\n",
       "           1.7422e-02, -1.3873e-02,  1.4859e-02, -6.5310e-02, -1.0105e-02,\n",
       "          -8.2604e-03,  9.2569e-03,  8.3488e-03,  1.4733e-02, -2.0265e-03,\n",
       "          -3.8639e-03, -1.2405e-02,  2.3938e-03,  5.5433e-02,  6.8165e-02,\n",
       "          -5.6874e-02, -1.0838e-02, -2.0363e-02, -3.8325e-02,  6.3800e-02,\n",
       "           1.3663e-02,  2.2237e-02]], grad_fn=<DivBackward0>),\n",
       " BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "          ...,\n",
       "          [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "          [ 1.0118, -0.6701,  1.7742,  ..., -0.1556, -0.0250, -1.5062],\n",
       "          [-0.5152,  0.1658,  0.8876,  ..., -0.0675, -0.4551, -1.7960]],\n",
       " \n",
       "         [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "          ...,\n",
       "          [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "          [ 0.2369, -0.5127,  1.7603,  ...,  1.3663,  1.1173,  0.3795],\n",
       "          [ 0.4128,  0.0553,  0.3044,  ...,  1.7624,  1.2203, -0.6843]],\n",
       " \n",
       "         [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "          ...,\n",
       "          [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "          [ 2.0956, -0.8056,  0.6844,  ...,  1.2079,  0.3350, -0.6070],\n",
       "          [ 1.9822, -1.3388,  0.5699,  ...,  0.7120, -0.4831, -1.0241]],\n",
       " \n",
       "         [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "          [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "          [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "          ...,\n",
       "          [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "          [ 2.3436, -1.2052,  0.2522,  ..., -0.7049, -0.3731, -0.7614],\n",
       "          [ 1.9001, -1.7980,  0.6247,  ...,  0.4795, -0.5519, -1.0527]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.5152,  0.1658,  0.8876,  ..., -0.0675, -0.4551, -1.7960],\n",
       "         [ 0.4128,  0.0553,  0.3044,  ...,  1.7624,  1.2203, -0.6843],\n",
       "         [ 1.9822, -1.3388,  0.5699,  ...,  0.7120, -0.4831, -1.0241],\n",
       "         [ 1.9001, -1.7980,  0.6247,  ...,  0.4795, -0.5519, -1.0527]],\n",
       "        grad_fn=<IndexBackward0>), hidden_states=None, attentions=None),\n",
       " BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.3185,  0.0538,  0.3447,  ..., -0.1289, -0.0099,  0.0779],\n",
       "          [ 0.0422,  0.4270, -0.6097,  ...,  0.1117,  0.6963,  0.2856],\n",
       "          [-0.0180,  0.5264, -0.5261,  ..., -0.5832,  0.9310,  0.2632],\n",
       "          ...,\n",
       "          [ 0.4886,  0.5699, -0.2275,  ...,  0.2783,  0.0184,  0.3018],\n",
       "          [ 0.6752,  0.6685, -0.4520,  ...,  0.1116, -0.0339,  0.1514],\n",
       "          [ 0.7083,  0.9596, -0.5510,  ...,  0.3479,  0.1869, -0.0969]]],\n",
       "        grad_fn=<AddBackward0>), pooler_output=tensor([[-6.4931e-01, -2.1808e-01,  1.0603e+00, -1.2804e-01,  1.2684e+00,\n",
       "           2.4310e-01,  8.1382e-01, -1.4741e+00,  1.5200e-01,  2.2943e-01,\n",
       "          -1.8423e-01,  2.8962e-01,  1.4856e+00,  1.7450e+00, -2.7218e-01,\n",
       "           9.5349e-01, -4.4686e-02,  5.7977e-01,  4.0834e-02,  8.1211e-01,\n",
       "           1.2075e+00,  2.6346e-01,  6.1218e-01,  3.4391e-01, -8.7725e-01,\n",
       "           8.5379e-01,  2.9744e-01, -3.6806e-01,  7.0113e-01,  2.7196e+00,\n",
       "          -3.7928e+00, -8.6323e-01,  2.5939e-01,  1.2585e-01,  1.1629e+00,\n",
       "           6.8983e-01, -5.3149e-01, -4.3245e-02,  1.5701e+00,  4.5969e-01,\n",
       "           5.1216e-01, -1.6401e-01, -8.0104e-01,  9.8964e-01, -9.7279e-01,\n",
       "          -6.2832e-01,  1.2720e+00, -9.7392e-01,  2.1876e-01,  1.0713e-02,\n",
       "           1.3150e+00,  2.3782e+00,  4.8787e-01,  5.9749e-01,  1.5226e-01,\n",
       "           4.4074e-01,  1.3224e+00, -1.1202e+00,  2.1882e-01,  2.2049e-02,\n",
       "          -1.2607e-01,  1.7444e+00,  3.7521e-02,  5.7139e-01,  3.2406e-01,\n",
       "          -7.9040e-01,  4.4274e-01,  1.0266e+00, -4.8321e-01, -6.1237e-01,\n",
       "           1.6941e-01,  5.7682e-01, -1.5059e-01,  4.9862e-01, -3.3368e-01,\n",
       "          -8.9357e-01, -8.2484e-01, -2.9194e-01, -1.4589e+00,  5.2234e-01,\n",
       "           8.5815e-01,  4.8760e-01, -1.8133e-01, -1.3593e+00,  8.8837e-01,\n",
       "           5.4891e-01, -4.2876e-01, -2.5265e-01,  1.4828e+00,  9.5318e-01,\n",
       "           6.1933e-01,  9.8972e-01,  1.0197e+00,  8.3473e-01,  4.4952e-01,\n",
       "          -9.3585e-01, -6.6950e-01,  3.5988e-01,  7.1676e-02,  7.7942e-01,\n",
       "           2.1667e-01,  1.3675e+00, -1.4594e+00, -7.5030e-02, -2.6625e-01,\n",
       "           3.4530e-01, -1.3455e+00,  2.8430e-02,  2.6190e-02, -4.3786e-01,\n",
       "          -1.0853e+00,  9.7758e-02,  1.6009e+00, -7.3719e-01,  1.3115e+00,\n",
       "          -1.2873e+00, -1.1843e+00, -2.7155e+00,  6.2686e-01,  7.3669e-01,\n",
       "           1.0883e+00,  6.2853e-01,  1.5783e-01,  6.6851e-01, -5.8459e-01,\n",
       "           4.3762e-01,  3.3354e-01,  2.9556e-01, -9.8295e-01,  8.2073e-01,\n",
       "          -1.5019e-01, -2.3948e+00,  3.9715e-01,  2.5749e-01,  2.2020e-01,\n",
       "           8.1920e-01,  1.7182e-01,  5.4433e-01,  1.3204e+00, -2.6168e-01,\n",
       "           1.1952e+00, -7.2855e-01,  7.0619e-01, -7.1360e-01,  1.5722e+00,\n",
       "          -3.1162e-01,  3.0663e-01,  1.0698e+00, -9.3684e-01,  9.7514e-01,\n",
       "          -8.1984e-01, -2.0876e-01, -5.9431e-01, -1.0054e+00,  3.5291e-01,\n",
       "          -6.8374e-01,  1.7413e+00, -1.0318e+00,  1.7482e-01,  1.0775e+00,\n",
       "          -5.6423e-01,  1.6336e+00,  6.0666e-01, -5.8743e-02, -6.4454e-01,\n",
       "           8.2667e-01, -6.0351e-01,  1.0696e-01,  3.4180e-01,  1.0093e+00,\n",
       "           1.2404e-01,  7.6901e-01,  8.7204e-01, -2.1550e-01,  8.2757e-01,\n",
       "          -2.7990e-01,  4.4984e-02, -1.0219e+00,  6.7778e-01,  1.2284e+00,\n",
       "           1.1315e-01,  3.9323e-01,  1.8496e-01,  4.5309e-01,  3.3629e-01,\n",
       "           9.5004e-01, -1.2970e+00,  1.2495e+00,  5.0552e-01,  1.8705e+00,\n",
       "           8.5702e-01,  2.3013e+00,  5.7727e-01,  1.4745e-01,  2.0101e-01,\n",
       "           1.2847e+00,  5.5599e-01,  5.0800e-01,  2.0741e-01,  1.1691e+00,\n",
       "          -4.3215e-02, -6.5683e-01,  7.4380e-01, -3.0333e-01, -3.7475e-01,\n",
       "           5.3126e-01, -3.2196e-01,  2.6884e-01,  2.4031e-01,  1.8683e+00,\n",
       "           1.0930e-01, -3.5781e-01, -4.1456e-01, -5.5218e-02, -1.7815e-01,\n",
       "          -5.5513e-01, -1.4366e-01,  4.9626e-01,  1.7628e+00, -8.5921e-01,\n",
       "          -2.0753e+00, -7.7307e-01, -7.8154e-01,  1.0726e-01, -2.2348e+00,\n",
       "          -2.7167e-01,  2.1084e+00,  1.7983e-01,  4.1818e-01, -8.0186e-01,\n",
       "          -1.4628e+00,  5.3362e-02,  2.4419e+00,  4.8001e-01, -2.1435e-01,\n",
       "           1.0423e+00, -4.6578e-01, -5.4335e-01,  6.9422e-01, -4.1962e-01,\n",
       "           1.2089e+00, -2.5106e-01,  1.0916e+00,  3.6695e-01,  8.3714e-01,\n",
       "          -4.0056e-01,  7.9348e-01, -1.7505e-02,  3.5888e-01,  1.1705e+00,\n",
       "           1.9635e+00, -2.9024e-01,  4.1832e-01,  1.0057e+00, -1.3966e+00,\n",
       "           6.1054e-01,  6.8946e-01,  2.2160e-01,  1.2913e+00,  5.2415e-01,\n",
       "           3.9889e-01,  5.4501e-01, -2.1316e+00, -6.5661e-01,  1.4866e+00,\n",
       "           8.6172e-01, -7.9171e-01, -6.6012e-01,  3.7298e-01, -5.2480e-01,\n",
       "          -1.8395e-01,  4.7233e-01, -6.4987e-01,  7.5299e-01,  1.0129e+00,\n",
       "           3.5528e-01,  1.2908e+00,  3.3514e-01,  3.1785e-01,  1.1967e+00,\n",
       "           1.8537e+00,  5.3015e-01, -1.8266e-01,  8.7088e-01, -2.9994e-01,\n",
       "          -4.7983e-01,  1.1410e+00,  2.3862e-01,  1.0227e+00,  1.3283e+00,\n",
       "           3.3465e-01,  2.4013e-01, -8.2542e-02, -2.1580e+00, -5.2514e-01,\n",
       "           1.2529e+00, -6.2906e-01, -1.9813e+00,  1.1691e+00,  4.3718e-01,\n",
       "           1.0210e+00, -2.0737e+00, -1.0673e-01,  6.1775e-01, -4.4400e-01,\n",
       "          -6.9510e-01,  6.2940e-01, -5.6270e-01,  6.6833e-01,  1.1007e+00,\n",
       "          -9.4858e-01, -4.3008e-02, -5.7673e-01,  3.0451e-01,  3.3780e-01,\n",
       "           1.1247e-01,  7.4604e-01,  9.7159e-01,  7.2997e-01, -6.9292e-01,\n",
       "          -6.5092e-01,  5.2195e-02, -3.0001e-01,  4.4271e-01, -6.3647e-01,\n",
       "          -1.9993e-01,  2.4186e-01, -8.6446e-01,  1.5330e+00,  3.4875e-01,\n",
       "          -1.1008e-01, -1.0299e+00, -5.0477e-01,  1.3907e+00,  1.3271e+00,\n",
       "           2.0640e-02, -6.7576e+00,  2.9356e-01,  4.1211e-01, -9.4664e-01,\n",
       "           1.2201e-02, -2.7529e+00,  1.6554e-01,  1.9883e+00,  1.8229e+00,\n",
       "          -7.8375e-01,  1.5287e+00,  7.7160e-01, -8.4259e-01,  6.1128e-01,\n",
       "           1.0010e-01, -2.8586e-01, -3.9626e-01, -1.4972e+00,  5.5895e-01,\n",
       "          -9.3560e-01, -2.7050e-01, -1.4946e+00,  1.0994e-01,  4.9101e-01,\n",
       "           5.5781e-01,  1.4250e+00,  9.3037e-01,  1.0601e-01,  1.2645e+00,\n",
       "           2.8066e-01,  3.3155e-01,  4.1834e-01,  4.6838e-01, -1.3763e+00,\n",
       "          -9.9808e-01,  6.5111e-01,  7.4206e-01, -6.5725e-01, -6.4923e-01,\n",
       "          -1.8402e-01, -9.4923e-01, -9.1509e-02,  1.1014e+00, -1.5680e+00,\n",
       "           1.2466e+00,  1.6875e+00, -2.3158e-01,  9.3572e-01,  2.9360e-01,\n",
       "           3.0946e+00, -2.0275e-01,  8.7561e-01,  2.1620e+00, -5.9409e-01,\n",
       "          -1.1898e+00,  1.8514e-01,  3.4000e-01, -2.2523e-02, -2.3821e+00,\n",
       "           1.3706e+00, -2.3696e-01,  1.0773e-01,  1.5708e-01, -9.3296e-01,\n",
       "           3.2917e-02, -1.0535e-01, -2.4021e+00, -9.3220e-02,  4.0416e-01,\n",
       "           2.2130e-01, -1.9789e-01,  1.2519e-01,  2.0524e-01,  9.0996e-01,\n",
       "          -7.7788e-01,  1.5046e-01,  1.3717e+00, -1.2219e-01, -1.3654e+00,\n",
       "           4.7863e-01, -1.4362e+00,  1.0471e-01,  1.0187e+00,  2.5158e-01,\n",
       "          -7.1365e-02,  1.4016e+00,  1.2723e+00, -3.8065e-02,  4.6527e-02,\n",
       "          -1.8075e-01, -5.4153e-01, -1.5012e-01,  1.4068e+00, -5.5706e-01,\n",
       "          -5.2973e-01,  4.0371e-01, -7.9331e-01,  1.3540e+00,  2.1942e-02,\n",
       "           7.2931e-01,  1.5367e+00,  9.1167e-01,  1.9266e-01,  1.3813e+00,\n",
       "           8.5430e-01,  2.5605e-01,  6.3271e-01, -5.0116e-01, -8.0944e-01,\n",
       "           4.7517e-01,  1.2997e+00,  6.0911e-01,  4.0679e-01, -2.1656e-01,\n",
       "          -4.0125e-01, -1.1563e+00,  2.1132e+00,  6.8344e-01, -1.7516e+00,\n",
       "          -7.3851e-01, -3.7211e-01,  1.7833e+00,  6.7763e-01, -6.6118e-01,\n",
       "          -1.5555e+00,  1.0296e+00,  1.2286e+00,  9.0153e-01,  6.6521e-01,\n",
       "           5.4997e-01,  1.4423e+00, -1.1013e+00, -4.6409e-01, -3.8389e-01,\n",
       "           2.2700e-01,  7.0610e-01, -5.8927e-01,  5.2816e-01,  1.4575e+00,\n",
       "          -5.1164e-02,  3.2043e-02,  4.6467e-01,  3.8812e-01, -4.4081e-01,\n",
       "           3.1322e-01, -2.0109e-01, -5.5897e-01,  3.2684e-01,  3.8008e+00,\n",
       "          -5.0244e-01, -2.0124e-01, -3.2776e-01,  1.1143e+00,  8.2776e-01,\n",
       "           1.2351e-01, -1.1965e+00,  1.0619e+00, -4.8175e-01, -2.9083e-01,\n",
       "           1.8587e+00,  3.2443e-01, -4.7090e-01,  1.2695e+00,  9.4336e-01,\n",
       "          -7.9107e-01,  7.8673e-01,  7.7235e-02, -6.4531e-01, -3.2026e-01,\n",
       "           1.9860e-01,  8.7904e-01, -9.3849e-01,  1.0005e-01, -6.5168e-01,\n",
       "           6.6346e-01, -1.0096e+00, -9.8021e-01,  2.1667e+00,  7.7221e-01,\n",
       "           1.1791e+00,  4.0361e-01,  5.5646e-01,  9.0877e-01,  7.7196e-01,\n",
       "          -3.7525e-01,  1.9900e-01, -4.7629e-01,  5.1140e-01,  3.0166e-02,\n",
       "           4.2966e-01,  1.8733e+00,  1.6087e+00,  4.8401e-01,  3.7807e-01,\n",
       "          -7.7894e-01,  1.8931e-01,  1.2339e+00, -1.3226e+00, -5.2823e-01,\n",
       "           8.5571e-01,  5.2963e-01,  1.5250e-01, -6.2353e-01, -8.1040e-03,\n",
       "           2.3309e+00, -2.2638e-01,  4.3483e-01, -4.1346e-01,  1.6015e-02,\n",
       "           4.5268e-01,  1.5746e+00,  8.1155e-01,  2.8702e-01,  6.1928e-01,\n",
       "          -1.4803e+00,  1.2691e+00,  1.1454e+00,  6.3037e-01,  5.5531e-01,\n",
       "           1.4146e+00, -7.9664e-01,  2.3070e-01,  1.2857e+00, -8.4133e-01,\n",
       "           1.2999e+00,  5.0504e-01,  1.6735e+00,  3.7233e-01,  2.5786e-01,\n",
       "           1.1853e+00,  7.3175e-01,  1.5035e-01,  8.2714e-02,  2.8599e-01,\n",
       "           5.2165e-01,  6.8166e-01,  2.7261e-01,  5.3533e-01,  4.9641e-01,\n",
       "           4.6445e-01, -1.2389e+00,  3.3471e+00,  6.2210e-01,  3.1573e-01,\n",
       "           7.6106e-01,  4.3286e-01,  9.3559e-02,  3.5843e-01, -5.1980e-01,\n",
       "          -4.9807e-01, -2.6830e-01,  3.8668e-01,  6.5956e-02,  8.7327e-01,\n",
       "           2.6455e-01,  4.5048e-01,  1.0795e+00,  4.5736e-01,  2.7196e-01,\n",
       "           6.0355e-01,  1.0566e+00, -1.5304e+00,  8.1854e-01, -4.9105e-01,\n",
       "          -1.4419e+00,  9.2470e-02,  1.0204e+00,  2.6887e-01,  3.2557e+00,\n",
       "          -3.8423e-01,  1.1493e+00,  5.0492e-01, -1.8822e-01,  5.2486e-01,\n",
       "          -7.5587e-01, -1.0777e-01,  1.3189e+00,  1.8998e-01, -2.9419e-01,\n",
       "           5.3295e-03,  3.1972e-01,  4.1056e-01,  1.2231e+00,  7.8606e-01,\n",
       "           7.7904e-01,  1.5449e+00,  6.2047e-02,  1.7617e+00, -7.5514e-01,\n",
       "           3.0861e+00,  1.3041e+00, -1.1203e-01,  3.2266e-01, -1.1759e+00,\n",
       "          -1.4473e+00, -3.5826e-01,  2.4646e-01, -6.2834e-01, -2.2586e-01,\n",
       "           4.1937e-01,  9.4264e-01,  9.7257e-01, -9.8546e-01,  8.4860e-02,\n",
       "           5.7984e-01,  6.6979e-01,  8.4118e-01, -2.0403e-02,  5.1616e-01,\n",
       "           1.2072e+00,  4.9944e-01,  1.4659e+00,  2.5176e-02,  2.9104e-01,\n",
       "          -2.0562e+00,  3.5076e-01,  1.3117e+00,  8.0226e-01, -2.1795e-01,\n",
       "          -6.2956e-02, -3.0300e-01, -6.5791e-01, -2.0028e-01, -1.3556e+00,\n",
       "           1.0552e+00,  4.5480e-01, -3.7363e-01,  7.1163e-01, -9.7190e-01,\n",
       "          -4.4890e-01,  1.0092e+00, -1.0391e+00,  1.8138e-01,  2.1943e-01,\n",
       "           1.2575e-01,  3.2744e-01, -1.1713e+00,  1.3440e-01,  2.1096e+00,\n",
       "           2.5663e-01, -5.8878e-01,  1.4146e+00, -4.7215e-02,  1.3459e+00,\n",
       "           4.8250e-01, -4.6943e-01,  8.0611e-01,  1.4595e+00,  1.5131e+00,\n",
       "          -2.4731e-03,  4.8940e-01,  9.5528e-01,  6.9458e-01, -2.9235e-02,\n",
       "          -4.8468e-01,  3.9750e-01, -1.5942e-01, -1.5681e+00,  9.6728e-01,\n",
       "          -2.2646e-01,  1.1474e+00, -5.1560e-01,  9.5968e-01, -3.2930e-01,\n",
       "           1.3909e-01, -6.5871e-01,  9.3148e-01,  2.0075e-02,  1.0169e+00,\n",
       "          -1.5313e+00,  9.4945e-01, -1.6397e+00, -1.1445e+00,  1.4233e+00,\n",
       "          -3.1156e-01,  3.6547e-01,  1.2362e+00,  1.1944e+00, -6.4503e-02,\n",
       "          -5.1200e-01, -1.7253e+00,  1.0768e-02,  5.1781e-01,  3.1868e-01,\n",
       "           7.7779e-01, -3.3074e-01,  9.5207e-01, -7.4989e-01,  2.6748e+00,\n",
       "           4.8090e-01, -5.8605e-01,  2.1185e-01,  1.1176e-01,  1.0051e-01,\n",
       "           4.4339e-01, -2.1216e-01, -8.6527e-01,  2.7338e-01, -1.0366e+00,\n",
       "           3.7783e-01,  3.6881e-01,  5.6963e-02, -4.2706e-01,  1.3623e-02,\n",
       "          -4.2439e-01,  2.8274e-01,  2.4807e-01,  1.0635e-01, -1.8441e+00,\n",
       "           4.4218e-01, -2.2771e-01,  1.2834e+00,  1.0783e+00,  4.3364e-01,\n",
       "           1.8753e-01, -5.2237e-01,  1.2046e+00,  6.6319e-01, -6.1130e-01,\n",
       "           2.5835e-01, -3.2563e-01,  7.0940e-01, -5.8969e-01,  2.0150e-01,\n",
       "           9.7268e-01, -7.6911e-03,  4.1308e-01,  5.1556e-01,  3.3374e-01,\n",
       "          -3.7422e-01,  1.5523e-01,  3.9434e-01]],\n",
       "        grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a warship\", \"a photo of a boy\", \"a photo of a girl\"]\n",
    "image = Image.open(f\"flickr8k/images/{image_path}\")\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs_no = clip_processor(text=text, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "inputs.keys(), torch.all(inputs.input_ids == inputs_no.input_ids)\n",
    "\n",
    "\n",
    "outputs = clip(**inputs)\n",
    "print(outputs.text_embeds.shape, outputs.image_embeds.shape, \n",
    "outputs.text_model_output[\"last_hidden_state\"].shape, outputs.text_model_output[\"pooler_output\"].shape, \n",
    "outputs.vision_model_output[\"last_hidden_state\"].shape, outputs.vision_model_output[\"pooler_output\"].shape, \n",
    ")\n",
    "outputs.text_embeds, outputs.image_embeds, outputs.text_model_output, outputs.vision_model_output\n",
    "# logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "# print(probs, text[probs.argmax(dim=-1)[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
