{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/image_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/text_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/captions.txt.zip\n",
    "!unzip -q image_all_final.pickle.zip \n",
    "!unzip -q text_all_final.pickle.zip\n",
    "!unzip -q captions.txt.zip\n",
    "!pip install transformers\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "!touch summary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xushitong/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import (\n",
    "  DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
    "  CLIPProcessor, CLIPModel as CLIP, CLIPConfig,\n",
    "  activations, PreTrainedTokenizer\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from spacy.lang.en import English\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"using device: \", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os,sys,humanize,psutil,GPUtil\n",
    "\n",
    "# Define function\n",
    "def mem_report():\n",
    "  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n",
    "  \n",
    "  GPUs = GPUtil.getGPUs()\n",
    "  for i, gpu in enumerate(GPUs):\n",
    "    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
    "\n",
    "mem_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and tokenizer\n",
    "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
    "  if tokenizer_class is not None:\n",
    "    tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
    "  if model_class is not None:\n",
    "    model = model_class.from_pretrained(name)\n",
    "    model.save_pretrained(f\"./models/{name}-local/\")\n",
    "\n",
    "# save_model_tokenizer(CLIPProcessor, CLIP, \"openai/clip-vit-base-patch32\")\n",
    "save_model_tokenizer(DistilBertTokenizer, DistilBertForMaskedLM, \"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_sum_batch_average(x_hat, x):\n",
    "  return (x_hat - x).abs().sum(dim=1).mean()\n",
    "\n",
    "# hyperparameters\n",
    "IN_CHANNEL = 768 # output dimention of embedding layer\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 16 # max text length\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCH_NUM = 5\n",
    "ROUNDING_WEIGHT = 0.3 # weight of rounding term, the probability of regenerated sequence \n",
    "# LOSS_FUNC = nn.functional.l1_loss\n",
    "LOSS_FUNC = series_sum_batch_average # loss function used between embedding \n",
    "CLIP_ADDING_METHOD = \"concat\" \"add\" # method CLIP feature is added to sequence of word embedding\n",
    "\n",
    "# diffusion hyperparameter\n",
    "BETA_MIN = 0.0001\n",
    "BETA_MAX = 0.02\n",
    "STEP_TOT = 1000 # total noise adding steps\n",
    "COSIN_SCHEDULE = True # if alpha sequence is scheduled in cosin instead of linear patten\n",
    "SAMPLE_SIZE = 100 # number of sample steps in each diffuse sequence\n",
    "X_0_PREDICTION = True # if model predicts x_0 or x_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use self defined tokenizer\n",
    "\n",
    "captions = pd.read_csv(\"captions.txt\")[\"caption\"]\n",
    "nlp = English()\n",
    "\n",
    "sentence_lst = []\n",
    "\n",
    "for sentences in captions:\n",
    "  word_lst = [x.text.lower() for x in nlp.tokenizer(sentences)]\n",
    "  spl = [[]]\n",
    "  for x, y in itertools.groupby(word_lst, lambda z: z == '.'):\n",
    "      spl[-1].extend(y)\n",
    "      if x: spl.append([])\n",
    "  sentence_lst.extend(spl[:-1])\n",
    "\n",
    "counter = Counter()\n",
    "for input_ids in sentence_lst:\n",
    "    counter.update(input_ids)\n",
    "vocab_dict = {'START': 0, 'END': 1, 'UNK':2, 'PAD':3}\n",
    "for k, v in counter.items():\n",
    "    if v > 10:\n",
    "      vocab_dict[k] = len(vocab_dict)\n",
    "VOCAB_SIZE = len(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kCLIPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, captions, tokenizer) -> None:\n",
    "    self.caption = captions\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    self.train_dataset = torch.utils.data.TensorDataset(torch.load(\"image_all_final.pickle\").to(device), torch.load(\"text_all_final.pickle\").to(device))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.caption)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_clip, text_clip = self.train_dataset[idx]\n",
    "    if isinstance(self.tokenizer, PreTrainedTokenizer):\n",
    "      tokens = self.tokenizer(text=self.caption.loc[idx], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "    else:\n",
    "      ids = [0] + [vocab_dict.get(x, vocab_dict['UNK']) for x in self.caption.loc[idx][:MAX_LENGTH-2]] + [1] \n",
    "      pad_length = max(0, MAX_LENGTH - len(ids))\n",
    "      tokens = dict()\n",
    "      tokens[\"input_ids\"] = torch.tensor(ids + [vocab_dict['UNK']] * pad_length)\n",
    "      tokens[\"attention_mask\"] = torch.tensor([1] * len(ids) + [0] * pad_length)\n",
    "\n",
    "    return {\n",
    "      \"image_clip\": image_clip, \n",
    "      \"text_clip\": text_clip, \n",
    "      \"input_ids\": tokens[\"input_ids\"].squeeze().to(device), \n",
    "      \"attention_mask\": tokens[\"attention_mask\"].squeeze().to(device)\n",
    "    }\n",
    "\n",
    "# TODO: COCO dataset\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"./tokenizers/distilbert-base-uncased-local/\", local_files_only=True)\n",
    "# tokenizer = vocab_dict\n",
    "\n",
    "train_dataset = Flickr8kCLIPDataset(pd.read_csv(\"captions.txt\")[\"caption\"], tokenizer)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertModel(nn.Module):\n",
    "  def __init__(self, embedding, projection, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      embedding: clip embedding module\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "\n",
    "    self.embedding = copy.deepcopy(embedding.requires_grad_(False))\n",
    "    self.projection = copy.deepcopy(projection.requires_grad_(False))\n",
    "    self.projection.bias.data = torch.zeros(self.projection.bias.data.shape, device=device).requires_grad_(False)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "    self.image_linear = nn.Linear(512, 768, device=device)\n",
    "    self.text_linear = nn.Linear(512, 768, device=device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return list(model.model.parameters()) + list(model.image_linear.parameters()) + list(model.text_linear.parameters())\n",
    "  \n",
    "  def forward(self, x, image_clip, text_clip, mask):\n",
    "    '''\n",
    "    input:\n",
    "      x: [x_t ... x_t], shape: [sample_size * batch_size, seq_len, dim]\n",
    "      image_clip, text_clip shape: [sample_size * batch_size, 1, clip_dim]\n",
    "      mask shape: [sample_size * batch_size, seq_len + 2]\n",
    "\n",
    "    return \n",
    "      vocab_out, shape: [sample_size * batch_size, seq_len, vocab_size]\n",
    "      feature_out, shape: [sample_size * batch_size, seq_len + 2, dim]\n",
    "    '''\n",
    "    sample_batch_multi, _, _ = x.shape\n",
    "\n",
    "    assert x.shape == (sample_batch_multi, MAX_LENGTH, 768)\n",
    "    assert image_clip.shape == text_clip.shape == (sample_batch_multi, 1, 512)\n",
    "    assert mask.shape == (sample_batch_multi, MAX_LENGTH + 2)\n",
    "    \n",
    "    x = torch.hstack([x, self.image_linear(image_clip), self.text_linear(text_clip)])\n",
    "    x_out = self.model(x, mask)[0]    \n",
    "\n",
    "    assert x_out.shape == (sample_batch_multi, MAX_LENGTH + 2, 768)\n",
    "    return self.projection(x_out[:, :-2, :]), x_out\n",
    "\n",
    "class DistilBertSmallModel(nn.Module):\n",
    "  def __init__(self, vocab_size, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      embedding: clip embedding module\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, IN_CHANNEL, device=device).requires_grad_(True)\n",
    "    self.lm_head = nn.Linear(IN_CHANNEL, vocab_size, bias=False, device=device).requires_grad_(True)\n",
    "\n",
    "    self.input_projection = nn.Linear(IN_CHANNEL, 768, device=device).requires_grad_(True)\n",
    "    self.output_projection = nn.Linear(768, IN_CHANNEL, device=device).requires_grad_(True)\n",
    "\n",
    "    self.image_linear = nn.Linear(512, 768, device=device).requires_grad_(True)\n",
    "    self.text_linear = nn.Linear(512, 768, device=device).requires_grad_(True)\n",
    "\n",
    "  def parameters(self):\n",
    "    return list(self.model.parameters()) \\\n",
    "        + list(self.embedding.parameters()) \\\n",
    "        + list(self.lm_head.parameters()) \\\n",
    "        + list(self.input_projection.parameters()) \\\n",
    "        + list(self.output_projection.parameters()) \\\n",
    "        + list(self.image_linear.parameters()) \\\n",
    "        + list(self.text_linear.parameters())\n",
    "  \n",
    "  def forward(self, x, image_clip, text_clip, mask):\n",
    "    '''\n",
    "    input:\n",
    "      x: [x_t ... x_t], shape: [sample_size * batch_size, seq_len, IN_CHANNEL]\n",
    "      image_clip, text_clip shape: [sample_size * batch_size, 1, clip_dim]\n",
    "      mask shape: [sample_size * batch_size, seq_len + 2]\n",
    "\n",
    "    return \n",
    "      vocab_out, shape: [sample_size * batch_size, seq_len, vocab_size]\n",
    "      feature_out, shape: [sample_size * batch_size, seq_len + 2, IN_CHANNEL]\n",
    "    '''\n",
    "    sample_batch_multi, _, _ = x.shape\n",
    "\n",
    "    assert x.shape == (sample_batch_multi, MAX_LENGTH, IN_CHANNEL)\n",
    "    assert image_clip.shape == text_clip.shape == (sample_batch_multi, 1, 512)\n",
    "    assert mask.shape == (sample_batch_multi, MAX_LENGTH + 2)\n",
    "    \n",
    "    x = torch.hstack([self.input_projection(x), self.image_linear(image_clip), self.text_linear(text_clip)])\n",
    "    x = self.model(x, mask)[0]\n",
    "    x_out = self.output_projection(x)\n",
    "\n",
    "    assert x_out.shape == (sample_batch_multi, MAX_LENGTH + 2, IN_CHANNEL)\n",
    "    return self.lm_head(x_out[:, :-2, :]), x_out\n",
    "    \n",
    "origin = DistilBertForMaskedLM.from_pretrained(\"./models/distilbert-base-uncased-local\", local_files_only=True).to(device)\n",
    "configuration = DistilBertConfig()\n",
    "model = DistilBertModel(origin.get_input_embeddings(), origin.get_output_embeddings(), config=configuration)\n",
    "\n",
    "# configuration = DistilBertConfig()\n",
    "# model = DistilBertSmallModel(VOCAB_SIZE, config=configuration)\n",
    "\n",
    "# parameter only include model, no embedding layer\n",
    "# trainer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COSIN_SCHEDULE:\n",
    "  def scheduler(t):\n",
    "    s = 0.008 # smalle value prevent beta_t too small, from Improved DDPM paper\n",
    "    return torch.cos(torch.pi / 2 * (t/STEP_TOT + s) / (1 + s)) ** 2\n",
    "  ts = torch.arange(STEP_TOT).to(device)\n",
    "  alpha_cumprod = scheduler(ts) / scheduler(torch.zeros(1, device=device))\n",
    "else:\n",
    "  betas = torch.hstack([torch.zeros(1), torch.linspace(BETA_MIN, BETA_MAX, STEP_TOT)]).to(device)\n",
    "  alphas = 1 - betas\n",
    "  alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
    "def diffuse_t(x, t):\n",
    "  '''\n",
    "  input:\n",
    "    x_shape: [batch_size, seq_len, IN_CHANNEL]\n",
    "    t shape: [sample num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "\n",
    "  return shape [sample_num * batch_size, seq_len, IN_CHANNEL]\n",
    "  '''\n",
    "  batch_size, seq_len, _ = x.shape\n",
    "  sample_shape = (t.numel(), *(1, ) * len(x.shape))\n",
    "\n",
    "  noise = torch.normal(0, 1, x.shape).to(device)\n",
    "  mean = torch.sqrt(alpha_cumprod[t].reshape(sample_shape)) * x \n",
    "  epsilon = noise * torch.sqrt(1 - alpha_cumprod[t]).reshape(sample_shape)\n",
    "  return (mean + epsilon).reshape((t.numel() * batch_size, seq_len, IN_CHANNEL))\n",
    "\n",
    "def generate_diffuse_pair(x_0, t, t_next=None):\n",
    "  '''\n",
    "  input:\n",
    "    x_0 shape: [batch_size, seq_len, IN_CHANNEL],\n",
    "    t shape: [sample_num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "  \n",
    "  return (net input, net target)\n",
    "    net input shape: [sample_num * batch_size, seq_len, IN_CHANNEL]\n",
    "    net target shape: if t_next is None then [batch_size, seq_len, IN_CHANNEL] else [sample_num * batch_size, seq_len, IN_CHANNEL]\n",
    "  '''\n",
    "  if X_0_PREDICTION:\n",
    "    # predict x_0\n",
    "    return (diffuse_t(x_0, t), x_0)\n",
    "\n",
    "  # predict x_{t_next}\n",
    "  return (diffuse_t(x_0, t), diffuse_t(x_0, t_next))\n",
    "\n",
    "def loss(model, x_t, x_1, x_tgt, x_0, image_clip, text_clip, mask, idx, loss_func):\n",
    "  ''' \n",
    "  input: \n",
    "    model, \n",
    "    x_t, x_tgt shape: [sample_num * batch_size, seq_len, IN_CHANNEL]\n",
    "      NOTE: x_tgt only used when X_0_PREDICTION is False\n",
    "    x_1, x_0 shape: [batch_size, seq_len, IN_CHANNEL]\n",
    "    image_clip, text_clip shape: [batch_size, clip_dim]\n",
    "    mask shape: [batch_size, seq_len]\n",
    "    idx shape: [batch_size, seq_len]\n",
    "    loss_func\n",
    "\n",
    "  return triple loss terms\n",
    "  '''\n",
    "  assert x_t.shape == (SAMPLE_SIZE * BATCH_SIZE, MAX_LENGTH, IN_CHANNEL)\n",
    "  assert x_1.shape == x_0.shape == (BATCH_SIZE, MAX_LENGTH, IN_CHANNEL)\n",
    "  assert image_clip.shape == text_clip.shape == (BATCH_SIZE, 512)\n",
    "  assert mask.shape == (BATCH_SIZE, MAX_LENGTH)\n",
    "  assert idx.shape == (BATCH_SIZE, MAX_LENGTH)\n",
    "\n",
    "  def concat_mask(mask, append, repeat_num):\n",
    "    '''\n",
    "    input: \n",
    "      mask shape: []\n",
    "      append shape: []\n",
    "      repeat_num: scalar, how many times mask is repeated, appended shape is repeated (repeat_num * mask.shape[0]) times\n",
    "\n",
    "    return:\n",
    "      mask concat append repeated repeat_num times\n",
    "    '''\n",
    "    concated_mask = mask.repeat((repeat_num, 1))\n",
    "    concated_mask = torch.hstack([concated_mask, append.repeat((concated_mask.shape[0], 1))])\n",
    "    return concated_mask\n",
    "\n",
    "  \n",
    "  repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_t.shape) - 1))\n",
    "  image_clip = image_clip.unsqueeze(1) # shape [ batch_size, 1, clip_dim]\n",
    "  text_clip = text_clip.unsqueeze(1) # shape same as above\n",
    "  if CLIP_ADDING_METHOD == \"concat\":\n",
    "    x_t_mask = concat_mask(mask, torch.tensor([1, 0], device=device), SAMPLE_SIZE)\n",
    "    x_1_mask = concat_mask(mask, torch.tensor([1, 0], device=device), 1)\n",
    "  elif CLIP_ADDING_METHOD == \"add\":\n",
    "    x_t_mask = mask.repeat((SAMPLE_SIZE, 1))\n",
    "    x_1_mask = mask\n",
    "    x_t = \n",
    "\n",
    "  # x_t restore loss\n",
    "  x_t_prob, x_t_hidden = model(x_t, image_clip.repeat(repeat_shape), text_clip.repeat(repeat_shape), x_t_mask)\n",
    "  if X_0_PREDICTION:\n",
    "    x_t_loss = loss_func(x_t_hidden[:, :-2, :], x_0.repeat(repeat_shape))\n",
    "  else:\n",
    "    assert x_tgt.shape == x_t.shape\n",
    "    x_t_loss = loss_func(x_t_hidden[:, :-2, :], x_tgt)\n",
    "\n",
    "  # x_1 restore loss\n",
    "  x_1_prob, x_1_hidden = model(x_1, image_clip, text_clip, x_1_mask)\n",
    "  x_1_loss = loss_func(x_1_hidden[:, :-2, :], x_0)\n",
    "\n",
    "  # output sequence probability loss, applied to both x_1 and x_t restore\n",
    "  idx = idx.unsqueeze(dim=-1)\n",
    "  x_t_prob_loss = -(nn.functional.softmax(x_t_prob, dim=-1)).gather(-1, idx.repeat(repeat_shape)).log().sum(dim=1).mean()\n",
    "  x_1_prob_loss = -(nn.functional.softmax(x_1_prob, dim=-1)).gather(-1, idx).log().sum(dim=1).mean()\n",
    "  \n",
    "  return x_t_loss, x_1_loss, ROUNDING_WEIGHT * (x_t_prob_loss + x_1_prob_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_ADDING_METHOD == \"add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample 0:   0%|          | 0/10113 [01:49<?, ?batch/s, prob_loss=8.66, tot_loss=12.9, x_1_loss=0.966, x_t_hidden=3.27]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     tepoch\u001b[39m.\u001b[39mset_postfix(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                        x_t_hidden\u001b[39m=\u001b[39mx_t_loss\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                        x_1_loss\u001b[39m=\u001b[39mx_1_loss\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                        prob_loss\u001b[39m=\u001b[39mprob_loss\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                        tot_loss\u001b[39m=\u001b[39ml\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mif\u001b[39;00m batch_num \u001b[39m%\u001b[39m \u001b[39m25\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m       summary\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msample \u001b[39m\u001b[39m{\u001b[39;00mbatch_num\u001b[39m}\u001b[39;00m\u001b[39m avg x_t_loss, x_1_loss, prob_loss: \u001b[39m\u001b[39m{\u001b[39;00macc_x_t \u001b[39m/\u001b[39m (batch_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00macc_x_1 \u001b[39m/\u001b[39m (batch_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00macc_prob \u001b[39m/\u001b[39m (sample_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X24sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m average x_t_loss, x_1_loss, prob_loss: \u001b[39m\u001b[39m{\u001b[39;00macc_x_t \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00macc_x_1 \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00macc_prob \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_num' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# training TODO: forward new model\n",
    "# model = torch.load(\"model-200-sample-pretrain-embedding.pickle\")\n",
    "# model.model.add_module(\"activation\", activations.GELUActivation())\n",
    "# trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "summary = open(\"temp.txt\", \"a\")\n",
    "\n",
    "model.train()\n",
    "print(\"start training\")\n",
    "for epoch in range(EPOCH_NUM):\n",
    "  acc_x_t = 0\n",
    "  acc_x_1 = 0\n",
    "  acc_prob = 0\n",
    "  with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch: \n",
    "    for batch_num, x in enumerate(tepoch):\n",
    "  # for batch_num, x in enumerate(train_loader):\n",
    "\n",
    "      x_0 = model.embedding(x[\"input_ids\"])\n",
    "      repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_0.shape) - 1))\n",
    "      t = torch.randint(0, STEP_TOT, repeat_shape, device=device)\n",
    "      \n",
    "      if X_0_PREDICTION:\n",
    "        x_t = diffuse_t(x_0, t)\n",
    "        x_tgt = None\n",
    "      else:\n",
    "        x_t, x_tgt = generate_diffuse_pair(x_0, t, torch.max(t - 30, torch.zeros(t.shape, device=device, dtype=torch.int64)))\n",
    "      x_1 = diffuse_t(x_0, torch.ones(1, dtype=torch.int64, device=device))\n",
    "\n",
    "      trainer.zero_grad()\n",
    "      x_t_loss, x_1_loss, prob_loss = loss(\n",
    "        model, \n",
    "        x_t, x_1, x_tgt, x_0, \n",
    "        x[\"image_clip\"], x[\"text_clip\"], \n",
    "        x[\"attention_mask\"], \n",
    "        x[\"input_ids\"], \n",
    "        LOSS_FUNC\n",
    "      )\n",
    "      \n",
    "      l = x_t_loss + x_1_loss + prob_loss\n",
    "      l.backward()\n",
    "      trainer.step()\n",
    "      \n",
    "      acc_x_t += x_t_loss \n",
    "      acc_x_1 += x_1_loss \n",
    "      acc_prob += prob_loss\n",
    "\n",
    "      tepoch.set_description(f\"batch {batch_num}\")\n",
    "      tepoch.set_postfix(\n",
    "                         x_t_hidden=x_t_loss.item(),\n",
    "                         x_1_loss=x_1_loss.item(),\n",
    "                         prob_loss=prob_loss.item(),\n",
    "                         tot_loss=l.item())\n",
    "\n",
    "      if batch_num % 25 == 0:\n",
    "        summary.write(f\"batch {batch_num} avg x_t_loss, x_1_loss, prob_loss: {acc_x_t / (batch_num + 1)}, {acc_x_1 / (batch_num + 1)}, {acc_prob / (batch_num + 1)}\\n\")\n",
    "      break\n",
    "\n",
    "  print(f\"epoch {epoch} average x_t_loss, x_1_loss, prob_loss: {acc_x_t / len(train_loader)}, {acc_x_1 / len(train_loader)}, {acc_prob / len(train_loader)}\")\n",
    "  break\n",
    "summary.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin text:  A little girl is sitting in front of a large painted rainbow .\n",
      "t = 500\n",
      "inferred:  [CLS] a little girl is sitting in front of a large white yellow. [SEP] [\n",
      "inferred:  [CLS] a little girl is sitting in front of a large white yellow. [SEP] [\n",
      "inferred:  [CLS] a little girl is sitting in front of a large white yellow. [SEP] [\n",
      "inferred:  [CLS] a little girl is sitting in front of a large down yellow. [SEP] [P\n",
      "inferred:  [CLS] a little girl is sitting in front of a large sand yellow. [SEP] [P\n",
      "text t effectiveness\n",
      "t:  1 restore:  [CLS] a little girl is sitting in front of a large over yellow. [SEP] [PAD]\n",
      "t:  101 restore:  [CLS] a little girl is sitting in front of a large of yellow. [SEP] [PAD]\n",
      "t:  201 restore:  [CLS] a little girl is sitting in front of a large over yellow. [SEP] [PAD]\n",
      "t:  301 restore:  [CLS] a little girl is sitting in front of a large at yellow. [SEP] [PAD]\n",
      "t:  401 restore:  [CLS] a little girl is sitting in front of a large on rides. [SEP] [PAD]\n",
      "t:  501 restore:  [CLS] a little girl is sitting in front of a large over men. [SEP] [PAD]\n",
      "t:  601 restore:  [CLS] a little girl is sitting in front of a large colorful men. [SEP] [PAD]\n",
      "t:  701 restore:  [CLS] a little girl is sitting in front of a large green.. [SEP] [PAD]\n",
      "t:  801 restore:  [CLS] a little girl is sitting in front of a large are riding. [SEP] [PAD]\n",
      "t:  901 restore:  [CLS] a little girl is sitting in front of a large over fence. [SEP] [PAD]\n",
      "t:  1001 restore:  [CLS] a small girl is sitting in front of a large white two. [SEP] [PAD]\n",
      "t:  1101 restore:  [CLS] a little girl is sitting in front of a large water men. [SEP] [PAD]\n",
      "t:  1201 restore:  [CLS] a little girl is sitting in front of a large over lake. [SEP] [PAD]\n",
      "t:  1301 restore:  [CLS] a little girl is sitting in front of a large at an. [SEP] [PAD]\n",
      "t:  1401 restore:  [CLS] a young girl is sitting in front of a large are soccer. [SEP] [PAD]\n",
      "t:  1501 restore:  [CLS] a little girl is walks in front of a large green child. [SEP] [PAD]\n",
      "t:  1601 restore:  [CLS] a little in of sitting in a of a on in grass. [SEP] [PAD]\n",
      "t:  1701 restore:  [CLS] a little girl on a shirt a a riding on and man. [SEP] [PAD]\n",
      "t:  1801 restore:  [CLS] the woman is is walking in front of a road in woman. [SEP] [PAD]\n",
      "t:  1901 restore:  [CLS] a little girl a a a a a a a a on. [SEP] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# trial on inference\n",
    "model = torch.load(\"model-200-sample-pretrain-embedding.pickle\")\n",
    "model.model.add_module(\"activation\", activations.GELUActivation())\n",
    "model.eval()\n",
    "idx = 11\n",
    "origin_text = train_dataset.caption.loc[idx]\n",
    "print(\"origin text: \", origin_text)\n",
    "\n",
    "sample = train_dataset[idx]\n",
    "image_clip = sample[\"image_clip\"][None, None, :]\n",
    "text_clip = sample[\"text_clip\"][None, None, :]\n",
    "x_0 = model.embedding(sample[\"input_ids\"].unsqueeze(0))\n",
    "t = 500\n",
    "print(f\"t = {t}\")\n",
    "x_t = diffuse_t(x_0, torch.tensor([t], dtype=torch.int64, device=device))\n",
    "mask = sample[\"attention_mask\"].unsqueeze(0)\n",
    "\n",
    "none_mask = torch.hstack([mask, torch.ones((mask.shape[0], 2), device=device)])\n",
    "text_mask = torch.hstack([mask, torch.tensor([1, 0], device=device).repeat((mask.shape[0], 1))])\n",
    "all_mask = torch.hstack([mask, torch.zeros((mask.shape[0], 2), device=device)])\n",
    "applied_mask = text_mask\n",
    "\n",
    "# multi-step inference\n",
    "restored = x_t\n",
    "for i in range(5):\n",
    "  out, restored = model(restored[:, :MAX_LENGTH, :], image_clip, text_clip, applied_mask)\n",
    "  print(\"inferred: \", train_dataset.tokenizer.decode(out.argmax(dim=-1)[0])[:len(origin_text) + 10])\n",
    "\n",
    "# effectiveness of model on large t\n",
    "print(\"text t effectiveness\")\n",
    "for i in range(1, STEP_TOT, 100):\n",
    "  x_t = diffuse_t(x_0, torch.tensor([i], dtype=torch.int64, device=device))\n",
    "  out, _ = model(x_t, image_clip, text_clip, applied_mask) \n",
    "\n",
    "  print(\"t: \", i, \"restore: \", train_dataset.tokenizer.decode(out.argmax(dim=-1)[0])[:len(origin_text) + 20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a warship\", \"a photo of a boy\", \"a photo of a girl\"]\n",
    "image = Image.open(\"Shropshire.jpeg\")\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs_no = clip_processor(text=text, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs.keys(), torch.all(inputs.input_ids == inputs_no.input_ids))\n",
    "\n",
    "outputs = clip.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "# outputs = clip(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], pixel_values=torch.zeros((3,1,1)))\n",
    "\n",
    "# outputs = clip(**inputs)\n",
    "print(outputs.text_embeds.shape, outputs.image_embeds.shape, \n",
    "outputs.text_model_output[\"last_hidden_state\"].shape, outputs.text_model_output[\"pooler_output\"].shape, \n",
    "outputs.vision_model_output[\"last_hidden_state\"].shape, outputs.vision_model_output[\"pooler_output\"].shape, \n",
    ")\n",
    "outputs.text_embeds, outputs.image_embeds, outputs.text_model_output, outputs.vision_model_output\n",
    "# logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "# print(probs, text[probs.argmax(dim=-1)[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(), \"model.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
