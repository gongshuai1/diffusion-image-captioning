{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/image_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/text_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/captions.txt.zip\n",
    "!unzip -q image_all_final.pickle.zip \n",
    "!unzip -q text_all_final.pickle.zip\n",
    "!unzip -q captions.txt.zip\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xushitong/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import (\n",
    "  DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
    "  CLIPProcessor, CLIPModel as CLIP, CLIPConfig\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"using device: \", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and tokenizer\n",
    "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
    "  if tokenizer_class is not None:\n",
    "    tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
    "  if model_class is not None:\n",
    "    model = model_class.from_pretrained(name)\n",
    "    model.save_pretrained(f\"./models/{name}-local/\")\n",
    "\n",
    "save_model_tokenizer(CLIPProcessor, CLIP, \"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 64 # max text length\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCH_NUM = 1\n",
    "ROUNDING_WEIGHT = 0.3 # weight of rounding term, the probability of regenerated sequence \n",
    "LOSS_FUNC = nn.functional.l1_loss\n",
    "\n",
    "# diffusion hyperparameter\n",
    "BETA_MIN = 0.0001\n",
    "BETA_MAX = 0.02\n",
    "STEP_TOT = 2000 # total noise adding steps\n",
    "COSIN_SCHEDULE = False # if alpha sequence is scheduled in cosin instead of linear patten\n",
    "SAMPLE_SIZE = 1 # number of sample steps in each diffuse sequence\n",
    "X_0_PREDICTION = True # if model predicts x_0 or x_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertModel(nn.Module):\n",
    "  def __init__(self, embedding, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      embedding: clip embedding module\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "\n",
    "    self.embedding = copy.deepcopy(embedding).to(device)\n",
    "    projection_weight = embedding.weight.data.clone().detach().to(device)\n",
    "    self.projection = nn.Linear(projection_weight.shape[1], projection_weight.shape[0])\n",
    "    self.projection.weight.data = projection_weight\n",
    "    self.projection.bias.data = torch.zeros(self.projection.bias.data.shape, device=device)\n",
    "    self.projection.requires_grad_(False)\n",
    "    self.embedding.requires_grad_(False)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.model.parameters()\n",
    "  \n",
    "  def forward(self, x, mask):\n",
    "    '''\n",
    "    input:\n",
    "      x: [x_t ... x_t, image_clip, text_clip], shape: [sample_size * batch_size, seq_len + 2, dim]\n",
    "\n",
    "    return \n",
    "      vocab_out, shape: [sample_size * batch_size, seq_len, vocab_size]\n",
    "      feature_out, shape: [sample_size * batch_size, seq_len + 2, dim]\n",
    "    '''\n",
    "    \n",
    "    x_out = self.model(x, mask)[0]    \n",
    "    return self.projection(x_out[:, :-2, :]), x_out\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"./tokenizers/openai/clip-vit-base-patch32-local\")\n",
    "clip = CLIP.from_pretrained(\"./models/openai/clip-vit-base-patch32-local\")\n",
    "\n",
    "configuration = DistilBertConfig(vocab_size=clip_processor.tokenizer.vocab_size, dim=clip.projection_dim, n_heads=8)\n",
    "model = DistilBertModel(clip.get_submodule(\"text_model.embeddings.token_embedding\"), config=configuration)\n",
    "\n",
    "# parameter only include model, no embedding layer\n",
    "# trainer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COSIN_SCHEDULE:\n",
    "  def scheduler(t):\n",
    "    s = 0.008 # smalle value prevent beta_t too small, from Improved DDPM paper\n",
    "    return torch.cos(torch.pi / 2 * (t/STEP_TOT + s) / (1 + s)) ** 2\n",
    "  ts = torch.arange(STEP_TOT).to(device)\n",
    "  alpha_cumprod = scheduler(ts) / scheduler(torch.zeros(1, device=device))\n",
    "else:\n",
    "  betas = torch.hstack([torch.zeros(1), torch.linspace(BETA_MIN, BETA_MAX, STEP_TOT)]).to(device)\n",
    "  alphas = 1 - betas\n",
    "  alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
    "def diffuse_t(x, t):\n",
    "  '''\n",
    "  input:\n",
    "    x_shape: [batch_size, seq_len, dim]\n",
    "    t shape: [sample num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "\n",
    "  return shape [sample_num * batch_size, seq_len, dim]\n",
    "  '''\n",
    "  batch_size, seq_len, dim = x.shape\n",
    "  sample_shape = (t.numel(), *(1, ) * len(x.shape))\n",
    "\n",
    "  noise = torch.normal(0, 1, x.shape).to(device)\n",
    "  mean = torch.sqrt(alpha_cumprod[t].reshape(sample_shape)) * x \n",
    "  epsilon = noise * torch.sqrt(1 - alpha_cumprod[t]).reshape(sample_shape)\n",
    "  return (mean + epsilon).reshape((t.numel() * batch_size, seq_len, dim))\n",
    "\n",
    "def generate_diffuse_pair(x_0, t, t_next=None):\n",
    "  '''\n",
    "  input:\n",
    "    x_0 shape: [batch_size, seq_len, dim],\n",
    "    t shape: [sample_num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "  \n",
    "  return (net input, net target)\n",
    "    net input shape: [sample_num * batch_size, seq_len, dim]\n",
    "    net target shape: if t_next is None then [batch_size, seq_len, dim] else [sample_num * batch_size, seq_len, dim]\n",
    "  '''\n",
    "  if X_0_PREDICTION:\n",
    "    # predict x_0\n",
    "    return (diffuse_t(x_0, t), x_0)\n",
    "\n",
    "  # predict x_{t_next}\n",
    "  return (diffuse_t(x_0, t), diffuse_t(x_0, t_next))\n",
    "\n",
    "def loss(model, x_t, x_1, x_tgt, x_0, image_clip, text_clip, mask, idx, loss_func):\n",
    "  ''' \n",
    "  input: \n",
    "    model, \n",
    "    x_t, x_tgt shape: [sample_num * batch_size, seq_len, dim]\n",
    "      NOTE: x_tgt only used when X_0_PREDICTION is False\n",
    "    x_1, x_0 shape: [batch_size, seq_len, dim]\n",
    "    image_clip, text_clip shape: [batch_size, dim]\n",
    "    mask shape: [batch_size, seq_len + 2]\n",
    "    idx shape: [batch_size, seq_len]\n",
    "    loss_func\n",
    "\n",
    "  return triple loss terms\n",
    "  '''\n",
    "  assert x_t.shape == (SAMPLE_SIZE * BATCH_SIZE, MAX_LENGTH, 512)\n",
    "  assert x_1.shape == x_0.shape == (BATCH_SIZE, MAX_LENGTH, 512)\n",
    "  assert image_clip.shape == text_clip.shape == (BATCH_SIZE, 512)\n",
    "  assert mask.shape == (BATCH_SIZE, MAX_LENGTH + 2)\n",
    "  assert idx.shape == (BATCH_SIZE, MAX_LENGTH)\n",
    "  \n",
    "  repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_t.shape) - 1))\n",
    "  image_clip = image_clip.unsqueeze(1) # shape [ batch_size, 1, dim]\n",
    "  text_clip = text_clip.unsqueeze(1) # shape same as above\n",
    "\n",
    "  # x_t restore loss\n",
    "  x_t_prob, x_t_hidden = model(torch.hstack([x_t, image_clip.repeat(repeat_shape), text_clip.repeat(repeat_shape)]), mask.repeat((SAMPLE_SIZE, 1)))\n",
    "  if X_0_PREDICTION:\n",
    "    x_t_loss = loss_func(x_t_hidden[:, :-2, :], x_0.repeat(repeat_shape))\n",
    "  else:\n",
    "    assert x_tgt.shape == x_t.shape\n",
    "    x_t_loss = loss_func(x_t_hidden[:, :-2, :], x_tgt)\n",
    "\n",
    "  # x_1 restore loss\n",
    "  x_1_prob, x_1_hidden = model(torch.hstack([x_1, image_clip, text_clip]), mask)\n",
    "  x_1_loss = loss_func(x_1_hidden[:, :-2, :], x_0)\n",
    "\n",
    "  # output sequence probability loss, applied to both x_1 and x_t restore\n",
    "  idx = idx.unsqueeze(dim=-1)\n",
    "  x_t_prob_loss = -(nn.functional.softmax(x_t_prob, dim=-1)).gather(-1, idx.repeat(repeat_shape)).log().mean()\n",
    "  x_1_prob_loss = -(nn.functional.softmax(x_1_prob, dim=-1)).gather(-1, idx).log().mean()\n",
    "  \n",
    "  return x_t_loss, x_1_loss, ROUNDING_WEIGHT * (x_t_prob_loss + x_1_prob_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kCLIPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, clip_processor) -> None:\n",
    "    self.caption = pd.read_csv(\"captions.txt\")\n",
    "    self.tokenizer = clip_processor\n",
    "\n",
    "    self.train_dataset = torch.utils.data.TensorDataset(torch.load(\"image_all_final.pickle\"), torch.load(\"text_all_final.pickle\"))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.caption)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_clip, text_clip = self.train_dataset[idx]\n",
    "    tokens = self.tokenizer(text=self.caption.loc[idx][\"caption\"], images=None, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    return {\n",
    "      \"image_clip\": image_clip, \n",
    "      \"text_clip\": text_clip, \n",
    "      \"input_ids\": tokens[\"input_ids\"].squeeze().to(device), \n",
    "      \"attention_mask\": tokens[\"attention_mask\"].squeeze().to(device)\n",
    "    }\n",
    "\n",
    "# TODO: COCO dataset\n",
    "\n",
    "train_dataset = Flickr8kCLIPDataset(clip_processor)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "# model = torch.load(\"model_continue1.pickle\")[\"net\"]\n",
    "# trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model.train()\n",
    "print(\"start training\")\n",
    "for epoch in range(EPOCH_NUM):\n",
    "  acc_loss = 0\n",
    "  with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch: \n",
    "    for sample_num, x in enumerate(tepoch):\n",
    "  # for x in train_loader:\n",
    "      x_0 = model.embedding(x[\"input_ids\"])\n",
    "      repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_0.shape) - 1))\n",
    "      t = torch.randint(0, STEP_TOT, repeat_shape, device=device)\n",
    "      if X_0_PREDICTION:\n",
    "        x_t = diffuse_t(x_0, t)\n",
    "        x_tgt = None\n",
    "      else:\n",
    "        x_t, x_tgt = generate_diffuse_pair(x_0, t, torch.max(t - 30, torch.zeros(t.shape, device=device, dtype=torch.int64)))\n",
    "      x_1 = diffuse_t(x_0, torch.ones(1, dtype=torch.int64, device=device))\n",
    "\n",
    "      trainer.zero_grad()\n",
    "      x_t_loss, x_1_loss, prob_loss = loss(\n",
    "        model, \n",
    "        x_t, x_1, x_tgt, x_0, \n",
    "        x[\"image_clip\"], x[\"text_clip\"], \n",
    "        torch.hstack([x[\"attention_mask\"], torch.ones((BATCH_SIZE, 2), device=device)]), \n",
    "        x[\"input_ids\"], \n",
    "        LOSS_FUNC\n",
    "      )\n",
    "      l = x_t_loss + x_1_loss + prob_loss\n",
    "      l.backward()\n",
    "      trainer.step()\n",
    "\n",
    "      acc_loss += l\n",
    "\n",
    "      tepoch.set_description(f\"sample {sample_num}\")\n",
    "      tepoch.set_postfix(\n",
    "                         x_t_hidden=x_t_loss.item(),\n",
    "                         x_1_loss=x_1_loss.item(),\n",
    "                         prob_loss=prob_loss.item(),\n",
    "                         tot_loss=l.item())\n",
    "      break\n",
    "\n",
    "  print(f\"epoch {epoch} average loss: {acc_loss / len(train_loader)}, last loss x_t_loss, x_1_loss, prob_loss: {x_t_loss.item(), x_1_loss.item(), prob_loss.item()}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial on inference\n",
    "model.eval()\n",
    "idx = 11\n",
    "origin_text = train_dataset.caption.loc[idx][\"caption\"]\n",
    "print(\"origin text: \", origin_text)\n",
    "\n",
    "sample = train_dataset[idx]\n",
    "for k in sample:\n",
    "  sample[k] = sample[k].unsqueeze(0)\n",
    "x_0 = model.embedding(sample[\"input_ids\"])\n",
    "t = 25\n",
    "print(f\"t = {t}\")\n",
    "x_t = diffuse_t(x_0, torch.tensor([t], dtype=torch.int64, device=device))\n",
    "\n",
    "# multi-step inference\n",
    "restored = x_t\n",
    "for i in range(5):\n",
    "  out, restored = model(restored, sample[\"attention_mask\"])\n",
    "  print(\"inferred: \", train_dataset.tokenizer.decode(out.argmax(dim=-1)[0])[:len(origin_text)])\n",
    "\n",
    "# effectiveness of model on large t\n",
    "print(\"text t effectiveness\")\n",
    "for i in range(1, 500, 25):\n",
    "  x_t = diffuse_t(x_0, torch.tensor([i], dtype=torch.int64, device=device))\n",
    "  out, _ = model(x_t, sample[\"attention_mask\"]) \n",
    "\n",
    "  print(\"t: \", i, \"restore: \", train_dataset.tokenizer.decode(out.argmax(dim=-1)[0])[:len(origin_text)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a warship\", \"a photo of a boy\", \"a photo of a girl\"]\n",
    "image = Image.open(\"Shropshire.jpeg\")\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs_no = clip_processor(text=text, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs.keys(), torch.all(inputs.input_ids == inputs_no.input_ids))\n",
    "\n",
    "outputs = clip.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "# outputs = clip(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], pixel_values=torch.zeros((3,1,1)))\n",
    "\n",
    "# outputs = clip(**inputs)\n",
    "print(outputs.text_embeds.shape, outputs.image_embeds.shape, \n",
    "outputs.text_model_output[\"last_hidden_state\"].shape, outputs.text_model_output[\"pooler_output\"].shape, \n",
    "outputs.vision_model_output[\"last_hidden_state\"].shape, outputs.vision_model_output[\"pooler_output\"].shape, \n",
    ")\n",
    "outputs.text_embeds, outputs.image_embeds, outputs.text_model_output, outputs.vision_model_output\n",
    "# logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "# print(probs, text[probs.argmax(dim=-1)[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu(), \"model.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
