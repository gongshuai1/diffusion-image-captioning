{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://storage.googleapis.com/kaggle-data-sets/771078/1328792/compressed/flickr8k.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220828%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220828T123221Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2da96ff4d583711c087af4482eb38f93034c0c9d78e743b4a3520b6480ef3d25998461288db8e5c23b18ad9270bec9d16d68ddd07aa8a20886f26f69f65afdb17b191cebda77a46b8d7fa9f78ab59359d935b544f1a434cfe472b943ed4293d74a1f821b4ca04c80d745791d9cafceb395cfe81006007afc9ab5f292ebb29954b1c7df69c47b556f1f71fef89ddfb924802df066f704916b81dfa36f978f0d70ebe0d71012b7c1e1d4937b3ce36520bfb71a818f449395297015e2dfec6ed2f29fb2b652469134a90fa4143e5095f77645632c6ebe4814fff501569badecab290d50d8316b3ea48667fdb7ab9f9fc86666e07026bf1470b41a0151de1b0e0323' -o flickr8k.zip\n",
    "!unzip -q flickr8k.zip -d flickr8k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xushitong/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import (\n",
    "  DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
    "  CLIPProcessor, CLIPModel as CLIP, CLIPConfig\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"using device: \", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and tokenizer\n",
    "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
    "  if tokenizer_class is not None:\n",
    "    tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
    "  if model_class is not None:\n",
    "    model = model_class.from_pretrained(name)\n",
    "    model.save_pretrained(f\"./models/{name}-local/\")\n",
    "\n",
    "save_model_tokenizer(CLIPProcessor, CLIP, \"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 128 # max text length\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCH_NUM = 1\n",
    "ROUNDING_WEIGHT = 0.3 # weight of rounding term, the probability of regenerated sequence \n",
    "\n",
    "# diffusion hyperparameter\n",
    "BETA_MIN = 0.0001\n",
    "BETA_MAX = 0.02\n",
    "STEP_TOT = 2000 # total noise adding steps\n",
    "COSIN_SCHEDULE = True # if alpha sequence is scheduled in cosin instead of linear patten\n",
    "SAMPLE_SIZE = 3 # number of sample steps in each diffuse sequence\n",
    "X_0_PREDICTION = True # if model predicts x_0 or x_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertModel(nn.Module):\n",
    "  def __init__(self, projection, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      projection: torch.tensor\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "\n",
    "    self.projection = nn.Linear(projection.shape[-2], projection.shape[-1], device=device)\n",
    "    self.projection.weight.data = projection\n",
    "    self.projection.bias.data = torch.zeros(self.projection.bias.data.shape, device=device)\n",
    "    self.projection.requires_grad_(False)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.model.parameters()\n",
    "  \n",
    "  def forward(self, x, mask):\n",
    "    '''\n",
    "    return \n",
    "      feature_out, shape: [batch_size, seq_len, dim]\n",
    "      vocab_out, shape: [batch_size, seq_len, vocab_size]\n",
    "    '''\n",
    "    \n",
    "    x_out = self.model(x, mask)[0]\n",
    "    return self.projection(x_out), x_out\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"./tokenizers/openai/clip-vit-base-patch32-local\")\n",
    "clip = CLIP.from_pretrained(\"./models/openai/clip-vit-base-patch32-local\")\n",
    "\n",
    "configuration = DistilBertConfig(vocab_size=clip_processor.tokenizer.vocab_size, dim=clip.projection_dim, n_heads=8)\n",
    "model = DistilBertModel(clip.get_submodule(\"text_model.embeddings.token_embedding\").weight.data, config=configuration)\n",
    "\n",
    "# parameter only include model, no embedding layer\n",
    "# trainer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD dataloader, slow load from origin data\n",
    "class Flickr8kCLIPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, dir, clip_processor, clip) -> None:\n",
    "    self.dir = dir\n",
    "    self.caption = pd.read_csv(f\"{dir}/captions.txt\")\n",
    "\n",
    "    self.clip = clip\n",
    "    self.clip_processor = clip_processor\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for b in batch:\n",
    "      images.append(Image.open(f\"{self.dir}/images/{b['image']}\"))\n",
    "      captions.append(b[\"caption\"])\n",
    "\n",
    "    inputs = self.clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True)\n",
    "    outputs = self.clip(**inputs)\n",
    "\n",
    "    return outputs.text_embeds, outputs.image_embeds\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.caption)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.caption.loc[idx]\n",
    "\n",
    "# try on load on request in colect func\n",
    "train_dataset = Flickr8kCLIPDataset(\"flickr8k/\", clip_processor, clip)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mimage_all_final.pickle\u001b[39m\u001b[39m\"\u001b[39m), torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mtext_all_final.pickle\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mBATCH_SIZE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.load(\"image_all_final.pickle\"), torch.load(\"text_all_final.pickle\"))\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/633 [00:22<1:59:09, 11.33s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_loader) \u001b[39mas\u001b[39;00m process:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=1'>2</a>\u001b[0m   \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(process):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m100\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=3'>4</a>\u001b[0m       \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 13\u001b[0m in \u001b[0;36mFlickr8kCLIPDataset.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=12'>13</a>\u001b[0m   \u001b[39mif\u001b[39;00m batch \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=14'>15</a>\u001b[0m   images\u001b[39m.\u001b[39mappend(Image\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdir\u001b[39m}\u001b[39;00m\u001b[39m/images/\u001b[39m\u001b[39m{\u001b[39;00mb[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=15'>16</a>\u001b[0m   captions\u001b[39m.\u001b[39mappend(b[\u001b[39m\"\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000023?line=17'>18</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_processor(text\u001b[39m=\u001b[39mcaptions, images\u001b[39m=\u001b[39mimages, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with tqdm.tqdm(train_loader) as process:\n",
    "  for i, _ in enumerate(process):\n",
    "    if i > 100:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_all = torch.tensor([]).reshape((0, 512))\n",
    "text_all = torch.tensor([]).reshape((0, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "start = 1000\n",
    "end = 1010\n",
    "cnt = 1\n",
    "with tqdm.tqdm(range(start, end)) as process:\n",
    "  for i in process:\n",
    "    image, text = train_dataset[i]\n",
    "    image_all = torch.vstack([image_all, image])\n",
    "    text_all = torch.vstack([text_all, text])\n",
    "cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--ip=127.0.0.1'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_all_temp = image_all.clone().detach()\n",
    "text_all_temp = text_all.clone().detach()\n",
    "print(f\"saving to cnt = {cnt}\")\n",
    "torch.save(image_all_temp, f\"image_all{cnt}.pickle\")\n",
    "torch.save(text_all_temp, f\"text_all{cnt}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_all_final = \n",
    "torch.save(image_all1, \"image_all1.pickle\")\n",
    "torch.save(text_all1, \"text_all1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(image_all, \"image_all.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = torch.utils.data.TensorDataset(image_all, text_all)\n",
    "small_dataloader = DataLoader(small_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 527.91it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm.tqdm(small_dataloader) as process:\n",
    "  for image, text in process:\n",
    "    continue\n",
    "    # image_all = torch.vstack([image_all, image])\n",
    "    # text_all = torch.vstack([text_all, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dict = torch.load(\"image_dict.pickle\")\n",
    "for k in image_dict:\n",
    "  break\n",
    "image_dict[k][\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TODO: COCO dataset'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TODO: COCO dataset'''\n",
    "\n",
    "# import torchvision.transforms as transforms\n",
    "# cap = CocoCaptions(root = 'dir where images are',\n",
    "#                         annFile = 'json annotation file',\n",
    "#                         transform=transforms.PILToTensor())\n",
    "\n",
    "# print('Number of samples: ', len(cap))\n",
    "# img, target = cap[3] # load 4th sample\n",
    "\n",
    "# print(\"Image Size: \", img.size())\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values']) tensor(True)\n",
      "tensor([[ 0.1555,  0.0733, -0.2448,  ..., -0.5327, -0.4588,  0.0346],\n",
      "        [-0.0583,  0.1287,  0.1936,  ..., -0.2737,  0.0333, -0.3535],\n",
      "        [-0.0936,  0.3672, -0.1155,  ..., -0.8218, -0.2455, -0.1423],\n",
      "        [-0.1143,  0.2267, -0.2819,  ..., -0.6799, -0.2747,  0.1227]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify pixel_values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mget_text_features(input_ids\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], attention_mask\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=11'>12</a>\u001b[0m outputs2 \u001b[39m=\u001b[39m clip(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs_no)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=12'>13</a>\u001b[0m outputs2\u001b[39m.\u001b[39mtext_model_output[\u001b[39m\"\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1024\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1024\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m   1025\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1031\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\n\u001b[1;32m   1032\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1033\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1038\u001b[0m )\n\u001b[1;32m   1040\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:769\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    766\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    772\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_layrnorm(hidden_states)\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify pixel_values"
     ]
    }
   ],
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a warship\", \"a photo of a boy\", \"a photo of a girl\"]\n",
    "image = Image.open(\"Shropshire.jpeg\")\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs_no = clip_processor(text=text, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs.keys(), torch.all(inputs.input_ids == inputs_no.input_ids))\n",
    "\n",
    "outputs = clip.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "# outputs = clip(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], pixel_values=torch.zeros((3,1,1)))\n",
    "\n",
    "# outputs = clip(**inputs)\n",
    "print(outputs.text_embeds.shape, outputs.image_embeds.shape, \n",
    "outputs.text_model_output[\"last_hidden_state\"].shape, outputs.text_model_output[\"pooler_output\"].shape, \n",
    "outputs.vision_model_output[\"last_hidden_state\"].shape, outputs.vision_model_output[\"pooler_output\"].shape, \n",
    ")\n",
    "outputs.text_embeds, outputs.image_embeds, outputs.text_model_output, outputs.vision_model_output\n",
    "# logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "# print(probs, text[probs.argmax(dim=-1)[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
