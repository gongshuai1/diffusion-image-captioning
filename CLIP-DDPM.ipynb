{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/image_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/text_all_final.pickle.zip\n",
    "!unzip -q image_all_final.pickle.zip \n",
    "!unzip -q text_all_final.pickle.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xushitong/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import (\n",
    "  DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
    "  CLIPProcessor, CLIPModel as CLIP, CLIPConfig\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"using device: \", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and tokenizer\n",
    "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
    "  if tokenizer_class is not None:\n",
    "    tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
    "  if model_class is not None:\n",
    "    model = model_class.from_pretrained(name)\n",
    "    model.save_pretrained(f\"./models/{name}-local/\")\n",
    "\n",
    "save_model_tokenizer(CLIPProcessor, CLIP, \"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 64 # max text length\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCH_NUM = 1\n",
    "ROUNDING_WEIGHT = 0.3 # weight of rounding term, the probability of regenerated sequence \n",
    "\n",
    "# diffusion hyperparameter\n",
    "BETA_MIN = 0.0001\n",
    "BETA_MAX = 0.02\n",
    "STEP_TOT = 2000 # total noise adding steps\n",
    "COSIN_SCHEDULE = True # if alpha sequence is scheduled in cosin instead of linear patten\n",
    "SAMPLE_SIZE = 3 # number of sample steps in each diffuse sequence\n",
    "X_0_PREDICTION = True # if model predicts x_0 or x_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change model as described in loss part\n",
    "class DistilBertModel(nn.Module):\n",
    "  def __init__(self, projection, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      projection: torch.tensor\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "\n",
    "    self.projection = nn.Linear(projection.shape[-2], projection.shape[-1], device=device)\n",
    "    self.projection.weight.data = projection\n",
    "    self.projection.bias.data = torch.zeros(self.projection.bias.data.shape, device=device)\n",
    "    self.projection.requires_grad_(False)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.model.parameters()\n",
    "  \n",
    "  def forward(self, x, mask):\n",
    "    '''\n",
    "    return \n",
    "      feature_out, shape: [batch_size, seq_len, dim]\n",
    "      vocab_out, shape: [batch_size, seq_len, vocab_size]\n",
    "    '''\n",
    "    \n",
    "    x_out = self.model(x, mask)[0]\n",
    "    return self.projection(x_out), x_out\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"./tokenizers/openai/clip-vit-base-patch32-local\")\n",
    "clip = CLIP.from_pretrained(\"./models/openai/clip-vit-base-patch32-local\")\n",
    "\n",
    "configuration = DistilBertConfig(vocab_size=clip_processor.tokenizer.vocab_size, dim=clip.projection_dim, n_heads=8)\n",
    "model = DistilBertModel(clip.get_submodule(\"text_model.embeddings.token_embedding\").weight.data, config=configuration)\n",
    "\n",
    "# parameter only include model, no embedding layer\n",
    "# trainer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COSIN_SCHEDULE:\n",
    "  def scheduler(t):\n",
    "    s = 0.008 # smalle value prevent beta_t too small, from Improved DDPM paper\n",
    "    return torch.cos(torch.pi / 2 * (t/STEP_TOT + s) / (1 + s)) ** 2\n",
    "  ts = torch.arange(STEP_TOT).to(device)\n",
    "  alpha_cumprod = scheduler(ts) / scheduler(torch.zeros(1, device=device))\n",
    "else:\n",
    "  betas = torch.hstack([torch.zeros(1), torch.linspace(BETA_MIN, BETA_MAX, STEP_TOT)]).to(device)\n",
    "  alphas = 1 - betas\n",
    "  alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
    "def diffuse_t(x, t):\n",
    "  '''\n",
    "  input:\n",
    "    x_shape: [batch_size, seq_len, dim]\n",
    "    t shape: [sample num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "\n",
    "  return shape [sample_num * batch_size, seq_len, dim]\n",
    "  '''\n",
    "  batch_size, seq_len, dim = x.shape\n",
    "  sample_shape = (t.numel(), *(1, ) * len(x.shape))\n",
    "\n",
    "  noise = torch.normal(0, 1, x.shape).to(device)\n",
    "  mean = torch.sqrt(alpha_cumprod[t].reshape(sample_shape)) * x \n",
    "  epsilon = noise * torch.sqrt(1 - alpha_cumprod[t]).reshape(sample_shape)\n",
    "  return (mean + epsilon).reshape((t.numel() * batch_size, seq_len, dim))\n",
    "\n",
    "def generate_diffuse_pair(x_0, t, t_next=None):\n",
    "  '''\n",
    "  input:\n",
    "    x_0 shape: [batch_size, seq_len, dim],\n",
    "    t shape: [sample_num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "  \n",
    "  return (net input, net target)\n",
    "    net input shape: [sample_num * batch_size, seq_len, dim]\n",
    "    net target shape: if t_next is None then [batch_size, seq_len, dim] else [sample_num * batch_size, seq_len, dim]\n",
    "  '''\n",
    "  if X_0_PREDICTION:\n",
    "    # predict x_0\n",
    "    return (diffuse_t(x_0, t), x_0)\n",
    "\n",
    "  # predict x_{t_next}\n",
    "  return (diffuse_t(x_0, t), diffuse_t(x_0, t_next))\n",
    "\n",
    "def loss(model, image_clip, text_clip, x_t, x_1, x_tgt, x_0, mask, idx, loss_func):\n",
    "  ''' \n",
    "  input: \n",
    "    model, input sequence: [x_t ... x_t, image_clip, text_clip], output at position [x_t ... x_t] is model prediction\n",
    "    image_clip, text_clip shape: [batch_size, dim]\n",
    "    x_t, x_tgt shape: [sample_num * batch_size, seq_len, dim]\n",
    "      NOTE: x_tgt only used when X_0_PREDICTION is False\n",
    "    x_1, x_0 shape: [batch_size, seq_len, dim]\n",
    "    mask shape: [batch_size, seq_len]\n",
    "    idx shape: [batch_size, seq_len]\n",
    "    loss_func\n",
    "\n",
    "  return triple loss terms\n",
    "  '''\n",
    "  \n",
    "  repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_t.shape) - 1))\n",
    "\n",
    "  # x_t restore loss\n",
    "  x_t_prob, x_t_hidden = model(x_t, mask.repeat(repeat_shape))\n",
    "  if X_0_PREDICTION:\n",
    "    x_t_loss = loss_func(x_t_hidden, x_0.repeat(repeat_shape))\n",
    "  else:\n",
    "    x_t_loss = loss_func(x_t_hidden, x_tgt)\n",
    "\n",
    "  # x_1 restore loss\n",
    "  x_1_prob, x_1_hidden = model(x_1, mask)\n",
    "  x_1_loss = loss_func(x_1_hidden, x_0)\n",
    "\n",
    "  # output sequence probability loss, applied to both x_1 and x_t restore\n",
    "  idx = idx.unsqueeze(dim=-1)\n",
    "  x_t_prob_loss = -(nn.functional.softmax(x_t_prob, dim=-1)).gather(-1, idx.repeat(repeat_shape)).log().mean()\n",
    "  x_1_prob_loss = -(nn.functional.softmax(x_1_prob, dim=-1)).gather(-1, idx).log().mean()\n",
    "  \n",
    "  return x_t_loss, x_1_loss, ROUNDING_WEIGHT * (x_t_prob_loss + x_1_prob_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kCLIPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, clip_processor) -> None:\n",
    "    self.caption = pd.read_csv(\"flickr8k/captions.txt\")\n",
    "    self.tokenizer = clip_processor.tokenizer\n",
    "\n",
    "    self.train_dataset = torch.utils.data.TensorDataset(torch.load(\"image_all_final.pickle\"), torch.load(\"text_all_final.pickle\"))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.caption)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_clip, text_clip = self.train_dataset[idx]\n",
    "    embeddings = self.tokenizer(self.caption.loc[idx][\"caption\"], return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    return image_clip, text_clip, embeddings[\"input_ids\"], embeddings[\"attention_mask\"]\n",
    "\n",
    "# TODO: change to COCO dataset\n",
    "\n",
    "train_dataset = Flickr8kCLIPDataset(clip_processor)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values']) tensor(True)\n",
      "tensor([[ 0.1555,  0.0733, -0.2448,  ..., -0.5327, -0.4588,  0.0346],\n",
      "        [-0.0583,  0.1287,  0.1936,  ..., -0.2737,  0.0333, -0.3535],\n",
      "        [-0.0936,  0.3672, -0.1155,  ..., -0.8218, -0.2455, -0.1423],\n",
      "        [-0.1143,  0.2267, -0.2819,  ..., -0.6799, -0.2747,  0.1227]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify pixel_values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mget_text_features(input_ids\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], attention_mask\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=11'>12</a>\u001b[0m outputs2 \u001b[39m=\u001b[39m clip(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs_no)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=12'>13</a>\u001b[0m outputs2\u001b[39m.\u001b[39mtext_model_output[\u001b[39m\"\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1024\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1024\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m   1025\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1031\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\n\u001b[1;32m   1032\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1033\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1038\u001b[0m )\n\u001b[1;32m   1040\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:769\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    766\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    772\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_layrnorm(hidden_states)\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify pixel_values"
     ]
    }
   ],
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a warship\", \"a photo of a boy\", \"a photo of a girl\"]\n",
    "image = Image.open(\"Shropshire.jpeg\")\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs_no = clip_processor(text=text, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs.keys(), torch.all(inputs.input_ids == inputs_no.input_ids))\n",
    "\n",
    "outputs = clip.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "# outputs = clip(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], pixel_values=torch.zeros((3,1,1)))\n",
    "\n",
    "# outputs = clip(**inputs)\n",
    "print(outputs.text_embeds.shape, outputs.image_embeds.shape, \n",
    "outputs.text_model_output[\"last_hidden_state\"].shape, outputs.text_model_output[\"pooler_output\"].shape, \n",
    "outputs.vision_model_output[\"last_hidden_state\"].shape, outputs.vision_model_output[\"pooler_output\"].shape, \n",
    ")\n",
    "outputs.text_embeds, outputs.image_embeds, outputs.text_model_output, outputs.vision_model_output\n",
    "# logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "# print(probs, text[probs.argmax(dim=-1)[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
