{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/image_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/text_all_final.pickle.zip\n",
    "!wget https://github.com/xu-shitong/flickr8k-CLIP-freature/raw/master/captions.txt.zip\n",
    "!unzip -q image_all_final.pickle.zip \n",
    "!unzip -q text_all_final.pickle.zip\n",
    "!unzip -q captions.txt.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xushitong/miniconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from transformers import (\n",
    "  DistilBertTokenizer, DistilBertForMaskedLM, DistilBertConfig,\n",
    "  CLIPProcessor, CLIPModel as CLIP, CLIPConfig\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"using device: \", dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained model and tokenizer\n",
    "def save_model_tokenizer(tokenizer_class, model_class, name):\n",
    "  if tokenizer_class is not None:\n",
    "    tokenizer = tokenizer_class.from_pretrained(name)\n",
    "    tokenizer.save_pretrained(f\"./tokenizers/{name}-local\")\n",
    "  if model_class is not None:\n",
    "    model = model_class.from_pretrained(name)\n",
    "    model.save_pretrained(f\"./models/{name}-local/\")\n",
    "\n",
    "save_model_tokenizer(CLIPProcessor, CLIP, \"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 64 # max text length\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCH_NUM = 1\n",
    "ROUNDING_WEIGHT = 0.3 # weight of rounding term, the probability of regenerated sequence \n",
    "LOSS_FUNC = nn.functional.l1_loss\n",
    "\n",
    "# diffusion hyperparameter\n",
    "BETA_MIN = 0.0001\n",
    "BETA_MAX = 0.02\n",
    "STEP_TOT = 2000 # total noise adding steps\n",
    "COSIN_SCHEDULE = True # if alpha sequence is scheduled in cosin instead of linear patten\n",
    "SAMPLE_SIZE = 3 # number of sample steps in each diffuse sequence\n",
    "X_0_PREDICTION = True # if model predicts x_0 or x_{t-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, trainer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertModel(nn.Module):\n",
    "  def __init__(self, embedding, config=None) -> None:\n",
    "    '''\n",
    "    inputs:\n",
    "      embedding: clip embedding module\n",
    "      config\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.model = DistilBertForMaskedLM(config).to(device)\n",
    "\n",
    "    self.embedding = copy.deepcopy(embedding).to(device)\n",
    "    projection_weight = embedding.weight.data.clone().detach()\n",
    "    self.projection = nn.Linear(projection_weight.shape[1], projection_weight.shape[0], device=device)\n",
    "    self.projection.weight.data = projection_weight\n",
    "    self.projection.bias.data = torch.zeros(self.projection.bias.data.shape, device=device)\n",
    "    self.projection.requires_grad_(False)\n",
    "    self.embedding.requires_grad_(False)\n",
    "    \n",
    "    self.model.set_input_embeddings(nn.Sequential())\n",
    "    self.model.set_output_embeddings(nn.Sequential())\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.model.parameters()\n",
    "  \n",
    "  def forward(self, x, mask):\n",
    "    '''\n",
    "    input:\n",
    "      x: [x_t ... x_t, image_clip, text_clip], shape: [sample_size * batch_size, seq_len + 2, dim]\n",
    "\n",
    "    return \n",
    "      vocab_out, shape: [sample_size * batch_size, seq_len, vocab_size]\n",
    "      feature_out, shape: [sample_size * batch_size, seq_len + 2, dim]\n",
    "    '''\n",
    "    \n",
    "    x_out = self.model(x, mask)[0]    \n",
    "    return self.projection(x_out[:, :-2, :]), x_out\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"./tokenizers/openai/clip-vit-base-patch32-local\")\n",
    "clip = CLIP.from_pretrained(\"./models/openai/clip-vit-base-patch32-local\")\n",
    "\n",
    "configuration = DistilBertConfig(vocab_size=clip_processor.tokenizer.vocab_size, dim=clip.projection_dim, n_heads=8)\n",
    "model = DistilBertModel(clip.get_submodule(\"text_model.embeddings.token_embedding\"), config=configuration)\n",
    "\n",
    "# parameter only include model, no embedding layer\n",
    "# trainer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49408, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = clip.get_submodule(\"text_model.embeddings.token_embedding\")\n",
    "embedding.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COSIN_SCHEDULE:\n",
    "  def scheduler(t):\n",
    "    s = 0.008 # smalle value prevent beta_t too small, from Improved DDPM paper\n",
    "    return torch.cos(torch.pi / 2 * (t/STEP_TOT + s) / (1 + s)) ** 2\n",
    "  ts = torch.arange(STEP_TOT).to(device)\n",
    "  alpha_cumprod = scheduler(ts) / scheduler(torch.zeros(1, device=device))\n",
    "else:\n",
    "  betas = torch.hstack([torch.zeros(1), torch.linspace(BETA_MIN, BETA_MAX, STEP_TOT)]).to(device)\n",
    "  alphas = 1 - betas\n",
    "  alpha_cumprod = torch.cumprod(alphas[:-1], 0)\n",
    "def diffuse_t(x, t):\n",
    "  '''\n",
    "  input:\n",
    "    x_shape: [batch_size, seq_len, dim]\n",
    "    t shape: [sample num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "\n",
    "  return shape [sample_num * batch_size, seq_len, dim]\n",
    "  '''\n",
    "  batch_size, seq_len, dim = x.shape\n",
    "  sample_shape = (t.numel(), *(1, ) * len(x.shape))\n",
    "\n",
    "  noise = torch.normal(0, 1, x.shape).to(device)\n",
    "  mean = torch.sqrt(alpha_cumprod[t].reshape(sample_shape)) * x \n",
    "  epsilon = noise * torch.sqrt(1 - alpha_cumprod[t]).reshape(sample_shape)\n",
    "  return (mean + epsilon).reshape((t.numel() * batch_size, seq_len, dim))\n",
    "\n",
    "def generate_diffuse_pair(x_0, t, t_next=None):\n",
    "  '''\n",
    "  input:\n",
    "    x_0 shape: [batch_size, seq_len, dim],\n",
    "    t shape: [sample_num] \n",
    "      NOTE: not necessary have hyperparameter sample_size number of element, to allow single diffuse generation\n",
    "  \n",
    "  return (net input, net target)\n",
    "    net input shape: [sample_num * batch_size, seq_len, dim]\n",
    "    net target shape: if t_next is None then [batch_size, seq_len, dim] else [sample_num * batch_size, seq_len, dim]\n",
    "  '''\n",
    "  if X_0_PREDICTION:\n",
    "    # predict x_0\n",
    "    return (diffuse_t(x_0, t), x_0)\n",
    "\n",
    "  # predict x_{t_next}\n",
    "  return (diffuse_t(x_0, t), diffuse_t(x_0, t_next))\n",
    "\n",
    "def loss(model, x_t, x_1, x_tgt, x_0, image_clip, text_clip, mask, idx, loss_func):\n",
    "  ''' \n",
    "  input: \n",
    "    model, \n",
    "    x_t, x_tgt shape: [sample_num * batch_size, seq_len, dim]\n",
    "      NOTE: x_tgt only used when X_0_PREDICTION is False\n",
    "    x_1, x_0 shape: [batch_size, seq_len, dim]\n",
    "    image_clip, text_clip shape: [batch_size, dim]\n",
    "    mask shape: [batch_size, seq_len + 2]\n",
    "    idx shape: [batch_size, seq_len]\n",
    "    loss_func\n",
    "\n",
    "  return triple loss terms\n",
    "  '''\n",
    "  assert x_t.shape == (SAMPLE_SIZE * BATCH_SIZE, MAX_LENGTH, 512)\n",
    "  assert x_1.shape == x_0.shape == (BATCH_SIZE, MAX_LENGTH, 512)\n",
    "  assert image_clip.shape == text_clip.shape == (BATCH_SIZE, 512)\n",
    "  assert mask.shape == (BATCH_SIZE, MAX_LENGTH + 2)\n",
    "  assert idx.shape == (BATCH_SIZE, MAX_LENGTH)\n",
    "  \n",
    "  repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_t.shape) - 1))\n",
    "  image_clip = image_clip.unsqueeze(1) # shape [ batch_size, 1, dim]\n",
    "  text_clip = text_clip.unsqueeze(1) # shape same as above\n",
    "\n",
    "  # x_t restore loss\n",
    "  x_t_prob, x_t_hidden = model(torch.hstack([x_t, image_clip.repeat(repeat_shape), text_clip.repeat(repeat_shape)]), mask.repeat((SAMPLE_SIZE, 1)))\n",
    "  if X_0_PREDICTION:\n",
    "    x_t_loss = loss_func(x_t_hidden[:, :-2, :], x_0.repeat(repeat_shape))\n",
    "  else:\n",
    "    assert x_tgt.shape == x_t.shape\n",
    "    x_t_loss = loss_func(x_t_hidden[:, :-2, :], x_tgt)\n",
    "\n",
    "  # x_1 restore loss\n",
    "  x_1_prob, x_1_hidden = model(torch.hstack([x_1, image_clip, text_clip]), mask)\n",
    "  x_1_loss = loss_func(x_1_hidden[:, :-2, :], x_0)\n",
    "\n",
    "  # output sequence probability loss, applied to both x_1 and x_t restore\n",
    "  idx = idx.unsqueeze(dim=-1)\n",
    "  x_t_prob_loss = -(nn.functional.softmax(x_t_prob, dim=-1)).gather(-1, idx.repeat(repeat_shape)).log().mean()\n",
    "  x_1_prob_loss = -(nn.functional.softmax(x_1_prob, dim=-1)).gather(-1, idx).log().mean()\n",
    "  \n",
    "  return x_t_loss, x_1_loss, ROUNDING_WEIGHT * (x_t_prob_loss + x_1_prob_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kCLIPDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, clip_processor) -> None:\n",
    "    self.caption = pd.read_csv(\"captions.txt\")\n",
    "    self.tokenizer = clip_processor\n",
    "\n",
    "    self.train_dataset = torch.utils.data.TensorDataset(torch.load(\"image_all_final.pickle\"), torch.load(\"text_all_final.pickle\"))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.caption)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image_clip, text_clip = self.train_dataset[idx]\n",
    "    tokens = self.tokenizer(text=self.caption.loc[idx][\"caption\"], images=None, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    return {\n",
    "      \"image_clip\": image_clip, \n",
    "      \"text_clip\": text_clip, \n",
    "      \"input_ids\": tokens[\"input_ids\"].squeeze(), \n",
    "      \"attention_mask\": tokens[\"attention_mask\"].squeeze()\n",
    "    }\n",
    "\n",
    "# TODO: COCO dataset\n",
    "\n",
    "train_dataset = Flickr8kCLIPDataset(clip_processor)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2529 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 66, 512]) torch.Size([48, 66])\n",
      "torch.Size([48, 66, 512]) torch.Size([49408, 512])\n",
      "torch.Size([16, 66, 512]) torch.Size([16, 66])\n",
      "torch.Size([16, 66, 512]) torch.Size([49408, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2529 [00:28<?, ?batch/s, prob_loss=6.09, tot_loss=7.63, x_1_loss=0.77, x_t_hidden=0.772]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 average loss: 0.0030178946908563375, last loss x_t_loss, x_1_loss, prob_loss: (0.7718897461891174, 0.7699123620986938, 6.090453624725342)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# model = torch.load(\"model_continue1.pickle\")[\"net\"]\n",
    "# trainer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "model.train()\n",
    "print(\"start training\")\n",
    "for epoch in range(EPOCH_NUM):\n",
    "  acc_loss = 0\n",
    "  with tqdm.tqdm(train_loader, unit=\"batch\") as tepoch: \n",
    "    for epoch, x in enumerate(tepoch):\n",
    "  # for x in train_loader:\n",
    "      x_0 = model.embedding(x[\"input_ids\"])\n",
    "      repeat_shape = (SAMPLE_SIZE, *(1, ) * (len(x_0.shape) - 1))\n",
    "      t = torch.randint(0, STEP_TOT, repeat_shape, device=device)\n",
    "      if X_0_PREDICTION:\n",
    "        x_t = diffuse_t(x_0, t)\n",
    "        x_tgt = None\n",
    "      else:\n",
    "        x_t, x_tgt = generate_diffuse_pair(x_0, t, torch.max(t - 30, torch.zeros(t.shape, device=device, dtype=torch.int64)))\n",
    "      x_1 = diffuse_t(x_0, torch.ones(1, dtype=torch.int64, device=device))\n",
    "\n",
    "      trainer.zero_grad()\n",
    "      x_t_loss, x_1_loss, prob_loss = loss(\n",
    "        model, \n",
    "        x_t, x_1, x_tgt, x_0, \n",
    "        x[\"image_clip\"], x[\"text_clip\"], \n",
    "        torch.hstack([x[\"attention_mask\"], torch.ones((BATCH_SIZE, 2), device=device)]), \n",
    "        x[\"input_ids\"], \n",
    "        LOSS_FUNC\n",
    "      )\n",
    "      l = x_t_loss + x_1_loss + prob_loss\n",
    "      l.backward()\n",
    "      trainer.step()\n",
    "\n",
    "      acc_loss += l\n",
    "\n",
    "      tepoch.set_description(f\"Epoch {epoch}\")\n",
    "      tepoch.set_postfix(\n",
    "                         x_t_hidden=x_t_loss.item(),\n",
    "                         x_1_loss=x_1_loss.item(),\n",
    "                         prob_loss=prob_loss.item(),\n",
    "                         tot_loss=l.item())\n",
    "      break\n",
    "\n",
    "  print(f\"epoch {epoch} average loss: {acc_loss / len(train_loader)}, last loss x_t_loss, x_1_loss, prob_loss: {x_t_loss.item(), x_1_loss.item(), prob_loss.item()}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin text:  A little girl is sitting in front of a large painted rainbow .\n"
     ]
    }
   ],
   "source": [
    "# trial on inference\n",
    "model.eval()\n",
    "origin_text = train_dataset.caption.loc[11][\"caption\"]\n",
    "text = train_dataset.tokenizer(text=origin_text, images=None, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "print(\"origin text: \", origin_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values']) tensor(True)\n",
      "tensor([[ 0.1555,  0.0733, -0.2448,  ..., -0.5327, -0.4588,  0.0346],\n",
      "        [-0.0583,  0.1287,  0.1936,  ..., -0.2737,  0.0333, -0.3535],\n",
      "        [-0.0936,  0.3672, -0.1155,  ..., -0.8218, -0.2455, -0.1423],\n",
      "        [-0.1143,  0.2267, -0.2819,  ..., -0.6799, -0.2747,  0.1227]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify pixel_values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=9'>10</a>\u001b[0m outputs \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mget_text_features(input_ids\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], attention_mask\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=11'>12</a>\u001b[0m outputs2 \u001b[39m=\u001b[39m clip(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs_no)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xushitong/diffusion-image-captioning/CLIP-DDPM.ipynb#ch0000017?line=12'>13</a>\u001b[0m outputs2\u001b[39m.\u001b[39mtext_model_output[\u001b[39m\"\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1024\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1024\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m   1025\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1031\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\n\u001b[1;32m   1032\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1033\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1038\u001b[0m )\n\u001b[1;32m   1040\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:769\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    766\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    772\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_layrnorm(hidden_states)\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify pixel_values"
     ]
    }
   ],
   "source": [
    "text = [\"a photo of a cat\", \"a photo of a warship\", \"a photo of a boy\", \"a photo of a girl\"]\n",
    "image = Image.open(\"Shropshire.jpeg\")\n",
    "# plt.imshow(image)\n",
    "# plt.show()\n",
    "inputs = clip_processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs_no = clip_processor(text=text, images=None, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs.keys(), torch.all(inputs.input_ids == inputs_no.input_ids))\n",
    "\n",
    "outputs = clip.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "# outputs = clip(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], pixel_values=torch.zeros((3,1,1)))\n",
    "\n",
    "# outputs = clip(**inputs)\n",
    "print(outputs.text_embeds.shape, outputs.image_embeds.shape, \n",
    "outputs.text_model_output[\"last_hidden_state\"].shape, outputs.text_model_output[\"pooler_output\"].shape, \n",
    "outputs.vision_model_output[\"last_hidden_state\"].shape, outputs.vision_model_output[\"pooler_output\"].shape, \n",
    ")\n",
    "outputs.text_embeds, outputs.image_embeds, outputs.text_model_output, outputs.vision_model_output\n",
    "# logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "# print(probs, text[probs.argmax(dim=-1)[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f93c46dd61ce6925df2c3958a0a88f8277015b6331ee5021fc1dcace5372220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
