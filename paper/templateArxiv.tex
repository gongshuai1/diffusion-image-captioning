\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{CLIP-DiffusionLM: Apply Diffusion Model on Image Captioning
%%%% Cite as
%%%% Update your official citation here when published 
\thanks{\textit{\underline{Citation}}: 
\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Shitong Xu \\
  Imperial College London \\
  \texttt{shitong.xu19@imperial.ac.uk} \\
  %% examples of more authors
  % \And
  % Author3 \\
  % Affiliation \\
  % Univ \\
  % City\\
  % \texttt{email@email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
In this work, we applied denoising diffusion probabilistic models to text generation in image captioning tasks. We show that our CLIP-diffusionLM *** On the flickr8k dataset, the model showed. By combining samples from flickr8k and flickr30k dataset, our model showed ... performance. In addition, the model achieved ... zero shot performance in COCO 2015 image caption task. Our code is available at ...

contribution: experiment on learning rate, optimizer, adding mechanism, cosine schedule, using classifier free or not. using model to predict image feature or not, scale up to larger dataset
\end{abstract}


% keywords can be removed
\keywords{Diffusion model \and CLIP \and Non auto-regressive generation}


\section{Introduction}
Image captioning has being a focus of research over the recent years. Previous text encoder used could be split to 2 general classes, which are autoregressive and non-autoregressive. Most of the sota models falls in the autoregressive class[]. However, autoregressive generation suffer from 1) the slow generation speed due to the generation step is token by token; and 2) not capable of refining prefix of sentences based on the later generated tokens. Multiple attempts have experimented using a non-autoregressive model in the text generation steps[]. The closest to our work is 2019 Masked Non-Autoregressive Image Captioning[], which used a BERT model as generator and involves 2 steps to refine the generated sequence. However, these work still used a discrete generation process, which means masking out certain tokens and train model to refine words in these certain positions. To the best of our knowledge, there has not been other work on generating caption embedding based on continuous generations steps. Our work aim at employing a model to refine generated token continuously on their embedding. In particular, we used pretrained CLIP model for generating image and text features, and distilbert model based on diffusion-lm for text sequence generation. Our contribution could be summarized as follow:
  - apply diffusion model in image captioning tasks 
  - experiments with multiple feature fusion methods, in particular the relative importance of restoring the token feature and certainty of generated sequence.



\section{Related Work}
\label{sec:headings}

\subsection{Autoregressive image captioning}
2015 deep caption with multimodal rnn[] proposed the mRNN model, which used CNN for image extraction and RNN for text generation. 2016 show attend tell[] employed the LSTM for text generation and exerimented on soft and hard attention for early fusion between image feature and text feature. Based on this early fusion method, 2016 knowing where to look[] experimented the late fusion of image and text features, allowing model to attend on either image or text modality during generation. 2017 cascade recurrent nn[] experimented on reversing the generated caption, allowing its backend model to refine the former tokens based on later caption tokens. 2018 gla[] used attention model to combine local and global feature from images, so that captions can more accurately identify occluded objects. Similarly, 2019 stack vs[] also used image features from both high and low generaliaty, and combined them using cross attention. Their work also involves multi step refining of the generated text caption. 2019 unsupervised image caption[] trained image caption in a GAN style, with a LSTM discriminator reproducing the original image feature from generated text sequence. Similrarly, 2019 mscap[] proposed GAN based method to train model predicting stylized text. Multiple discriminators are used to supervise if generated text captured image related feature, in the desired style, and similar to a caption made by human. 2019 Variational Autoencoder-Based Multiple ImageCaptioning Using a Caption Attention Map[] used variational auto encoder for extracting image information, their model allows multi caption generation by sampling from the learned image feature distribution, thus produce various captions for a single image. 2019 image caption generation with pos[] used POS tagging to help the generation of text. The image feature is used as additional input when the model is predicting tokens related to image-specific information, i.e. object, colour, relative position of objects. 2021 CLIP cap[] experimented on using pretrained CLIP image feature for sequence generation. The CLIP features are transformed to a sequence of token and used as prefix for a GPT-2 model in generation. 

\subsection{Non autoregressive image captioning}
In contrast, non-autoregressive models benefits from the attention models' ability to pass textural information in both direction during generation. The text generated in former timesteps could adjust based on text in later timesteps, thus is expected to achieve better performance. 2019 Masked Non-Autoregressive Image Captioning[] used BERT[] as text decoder and employed a 2 step generation method. Based on this work, Partially Non-Autoregressive Image Captioning [] and semi Non-Autoregressive Image Captioning[] partitioned the generated text in subgroups, words in the same group are generated non-autoregressively and different groups are generated in autoregressive way. Our method falls in this catogory and most close to the 2019 Masked Non-Autoregressive Image Captioning[]. The difference is we chose to use diffusion model as the non-autoregressive generation model. 2022 GRIT[] experimented changing the cross attention part of transformer decoder to use both Reginal feature from Faster RCNN and Grid features from swin transformer. 

\subsection{Diffusion models}
Diffusion models aims at training a model that denoise a feature from Gaussial noise to original features.
[] proposed the DDPM model to simplified the loss function by only letting models to predict the noise in generation steps, and proposed alternative loss functions by removing the weight terms. Based on DDPM, improved ddpm [] proposed several improvments based on DDPM, including setting variance to be learnable parameters, apply cosine instead of linear noise schedule, and speed up forward process by reducing forward steps. 

Diffusion model beat GAN[] is another work which proposed classifier guidance for improving generated image FID score. In a classifuer guided diffusion model, a classifier model is pretrained to predict noised images' object class. Noise on images are added as Gaussian noise to simulate the noise output in each step of diffusion generation. To guide the generation, the classifier provides gradient on which direction to optimise the generated image, so that the generate image resembles an object closer to the target class.

To avoid training classifier for guiding model, classifier free guidance technique is proposed Classifier-Free Diffusion Guidance[]. In classifier free guidance, the difference in output of generative model when provided with both guided and unguided context information is used as implicite guidance. Example of applying classifier free guidance includes GLIDE[], DALL-E2[], High-Resolution Image Synthesis With Latent Diffusion Models[]

Diffusion lm[] which is a recent work on applying continuous diffusion model on text generation, this paper provides various techniques to improve the performance of continuous diffusion model on text generation. 

ddim[] reduced the variance in forward process. The result showed that by reducing variance to 0, the deterministic model achieved higher FID score in image generation on both CIFAR10 and CelebA. 

By using diffusion model as text-to-image generation, DALL-E 2 and GLIDE model achieved significant image generation performance. Dall-e 2[] is a recent work on using CLIP and diffusion model for image generation task. The model used CLIP model for extracting feature from text, predict the corrisponding image CLIP feature through prior network, then use predicted image CLIP feature for final image generation. The model achieved significant novelty in generated images. The innovativity of generated of image from DALL-E 2 also provided us the inspiration to train a image-to-text model with diffusion model in generation step. 

\section{Background}

\subsection{Diffusion models}
The training of denoise diffusion probabilistic model involves generation of noised samples (forward process), and denoising based on model's output (backward process). Let x_0 be the original feature, the forward process incremently add noise to $x_0$ to generates a sequence of T noised features $[x_1, ... x_T]$. Each x_t at step t is sampled from probability distribution $q(x_t | x_{t-1}) = N(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I)$. From reparameterization trick, the x_t at any step t could be directly generate from x_0: $x_0 = ...$. The backward process is using a trained model with parameter $\theta$ to denoise the samples generated in the forward process. The training objective is to minimize the negative log-likelihood of generating x_0, that is to minimize $E(-\log(p_{\theta}(x_0))) = E(-\log(\int p_{\theta}(x_0, ..., x_T), d(x_1, ..., x_T))) = E(-\log(\int p_{\theta}(x_0, ..., x_T), d(x_1, ..., x_T)))$. Where $p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}, \mu_{theta}(x_t), ...)$ and $\mu_{\theta}$ is model's prediction on mean of $x_{t-1}$ conditioned on $x_t$. By modeling the backward process as a Markov Process, $p_{\theta}(x_1, ..., x_T)$ is simplified to $p_{\theta}(x_T) \prod_{t=1}^T p_{\theta}(x_{t-1} | x_t)$. From variational lower bound, $E(-\log(p_{\theta}(x_0))) \leq E_q[\log(\frac{p_{\theta}(x_0..x_T)}{q(x_1..x_T| x_0)})]$ $= E_[\log(p(x_T)) + \sum_{t = 1}^T\log(\frac{p_{\theta}(x_{t-1} | x_t)}{q(x_t | x_{t-1})})]$. From the work of [], expanding and reweighting each term of the negative log-likelihood gives a concise loss function $L = \sum_{t=1}^T E_{q(x_t | x_0)} \|\mu_{\theta}(x_t, t) - \mu_{x_t, x_0}\|^2$

Due to the large generation step number (T = 1000 as proposed in []), and the generation step being autoregressive on the denoised feature in the previous step, the reverse diffusion is significantly slower than the other generative models (... for gan and ... for diffusion). Multiple stetegies were proposed to accelerate the generation process. In Improved DDPM [] a subset of generation steps is selected. Model is trained to predict ... . In diffusion-lm the model is trained directly predict the x_0 instead of the intermediate steps containing noise. In our experiments, the ... showed better reproduce quality compared with... {}. In addition, by following the x_0 prediction method, our model could converge to a reasonable output in less than 5 diffusion steps. This step number is which is significantly less than autoregressive methods using an encoder-decoder structure. , which has generation steps propotional to the output sequense length. autoregressively apply the output of x_0 trained model further improved the performance. {}

Based on the propose from, we added an additional rounding term[] in our loss function, parameterized by $E_{p_{\theta}(\hat{x} | x_t)}-\log(p_{\theta}(w | \hat{x})) = E_{p_{\theta}(\hat{x} | x_t)}-\log(\prod_{i=1}^L p(w_i | \hat{x}_i))$. L represent the generated sequence length, w represent the gound truth sentence and $\hat{x}$ is the predicted sequence embedding from the input $x_t$. $p_{\theta}(w_i | \hat{x}_i)$ follows the softmax distribution. The training objection change to the following function:

$L = \sum_{t=1}^T E_{q(x_t | x_0)} \|\mu_{\theta}(x_t, t) - \mu_{x_t, x_0}\|^2 + -\log(p_{\theta}(w | \hat{x}))$

In our experiments, we found this term significantly influence the model performance{}.

\section{CLIP-DiffusionLM}


\section{Experiments}
Our model is based on Distilbert[] model, Distilbert is a model trained by distiling on BERT[] and has around 40% less parameter than BERT model. 

\subsection{fusion: concatenation or elementwise adding}

\subsection{relative importance of confidency of prediction and restored feature}
\subsection{guidance free training}
\subsection{learning rate}
\subsection{x_0 prediction or x_{t-n} prediction}
\subsection{number of x_t predictions}


\subsection{Conclusion}
We present the application of diffusion in image caption task, and proved its validity in limited dataset. Particularly, we identified the certainty term to be an important term in loss function to help model converge, and introduced the adaptive ratio adjustment to balance its importance with other terms. There are various improvements to the model and training process:
- Experiment on output the attention graph of the model, to check the model did focus on the correct region of the image.
- In various cases, the model failed to identify the correct object colour. For example, after correctly identify a girl wearing dress and a shirt, the model mistake the colour of shirt to the colour of the dress. 
- The output text sequence suffers from apparent grammar mistakes, for example, missing subject, repeated words. Additional supervision on the output text grammar might help model reduce such error.
- We trained on raw image data of the dataset. However, image argumentation has proven to be a valid method to improve the performance[]. Performing data argumentation might improve the model's generalizability and help reduce the wrong colour alignment problem as discussed above. 
We believe analysing and improving based on the above observations, diffusion as text generation step could achieve comparable or better performance than auto-regressive models. 




\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}


\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\section{Conclusion}
Your conclusion here

\section*{Acknowledgments}
This was was supported in part by......

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
